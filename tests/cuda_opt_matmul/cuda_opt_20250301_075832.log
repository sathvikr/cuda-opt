2025-03-01 07:58:32 |     INFO | 🚀 Starting CUDA optimization process
2025-03-01 07:58:32 |     INFO | 📖 Reading input CUDA kernel...
2025-03-01 07:58:32 |     INFO | Reading CUDA file from: tests/matmul.cu
2025-03-01 07:58:32 |     INFO | Starting optimization process (targeting 2 kernels)...
2025-03-01 07:58:32 |     INFO | 🔍 Searching for relevant papers on ArXiv...
2025-03-01 07:58:40 |     INFO | 
📝 Processing paper 1/2
2025-03-01 07:58:40 |     INFO | 📥 Downloading paper: 1910.08498.pdf
2025-03-01 07:58:42 |     INFO | 📄 Extracting text from PDF...
2025-03-01 07:58:47 |     INFO | ✓ Successfully processed paper (104196 chars extracted)
2025-03-01 07:58:47 |     INFO | 🤖 Implementing draft kernel optimization...
2025-03-01 07:58:47 |     INFO | Making request to DeepSeek API...
2025-03-01 07:59:48 |    ERROR | ❌ Network error during API request: HTTPSConnectionPool(host='api.deepseek.com', port=443): Read timed out.
2025-03-01 07:59:48 |    ERROR | Request payload: {'model': 'deepseek-reasoner', 'messages': [{'role': 'user', 'content': '\n                Using this paper as reference:\n                <paper>\n                A Benchmark Set of Highly-eﬃcient CUDA and OpenCL Kernels and its Dynamic\nAutotuning with Kernel Tuning Toolkit\nFilip Petroviˇca, David Stˇrel´aka,b, Jana Hozzov´aa, Jaroslav Ol’haa, Richard Trembeck´ya, Siegfried Benknerc, Jiˇr´ı\nFilipoviˇca,c,∗\naInstitute of Computer Science, Masaryk University, Botanick´a 68a, 60200 Brno, Czech Republic\nbSpanish National Centre for Biotechnology, Spanish National Research Council, Calle Darwin, 3, 28049 Madrid, Spain\ncFaculty of Computer Science, University of Vienna, W¨ahringer Str. 29, Vienna 1090, Austria\nAbstract\nIn recent years, the heterogeneity of both commodity and supercomputers hardware has increased sharply. Accelerators,\nsuch as GPUs or Intel Xeon Phi co-processors, are often key to improving speed and energy eﬃciency of highly-parallel\ncodes.\nHowever, due to the complexity of heterogeneous architectures, optimization of codes for a certain type of\narchitecture as well as porting codes across diﬀerent architectures, while maintaining a comparable level of performance,\ncan be extremely challenging. Addressing the challenges associated with performance optimization and performance\nportability, autotuning has gained a lot of interest. Autotuning of performance-relevant source-code parameters allows\nto automatically tune applications without hard coding optimizations and thus helps with keeping the performance\nportable. In this paper, we introduce a benchmark set of ten autotunable kernels for important computational problems\nimplemented in OpenCL or CUDA. Using our Kernel Tuning Toolkit, we show that with autotuning most of the kernels\nreach near-peak performance on various GPUs and outperform baseline implementations on CPUs and Xeon Phis. Our\nevaluation also demonstrates that autotuning is key to performance portability. In addition to oﬄine tuning, we also\nintroduce dynamic autotuning of code optimization parameters during application runtime. With dynamic tuning, the\nKernel Tuning Toolkit enables applications to re-tune performance-critical kernels at runtime whenever needed, for\nexample, when input data changes. Although it is generally believed that autotuning spaces tend to be too large to\nbe searched during application runtime, we show that it is not necessarily the case when tuning spaces are designed\nrationally. Many of our kernels reach near peak-performance with moderately sized tuning spaces that can be searched\nat runtime with acceptable overhead. Finally we demonstrate, how dynamic performance tuning can be integrated into\na real-world application from cryo-electron microscopy domain.\nKeywords:\ndynamic autotuning, opencl, cuda, performance optimization, autotuning benchmark set\n1. Introduction\nIn recent years, the acceleration of complex computa-\ntions using hardware accelerators have become much more\ncommon.\nCurrently, there are many devices developed\nby multiple vendors which diﬀer in hardware architecture,\nperformance, and other attributes.\nIn order to support\napplication development for these devices, several APIs\nsuch as OpenCL (Open Computing Language) or CUDA\n(Compute Uniﬁed Device Architecture) were designed. A\ncode written in those APIs is functionally portable: it can\n∗Corresponding author\nc⃝2020. This manuscript version is made available under the CC-\nBY-NC-ND 4.0 license http://creativecommons.org/licenses/by-nc-\nnd/4.0\nEmail addresses: fillo@mail.muni.cz (Filip Petroviˇc),\n373911@mail.muni.cz (David Stˇrel´ak), hozzova@mail.muni.cz\n(Jana Hozzov´a), 348646@mail.muni.cz (Jaroslav Ol’ha),\n422536@mail.muni.cz (Richard Trembeck´y),\nsiegfried.benkner@univie.ac.at (Siegfried Benkner),\nfila@mail.muni.cz (Jiˇr´ı Filipoviˇc)\nbe executed on various devices while producing the same\nresult. However, performance portability is often limited\ndue to the diﬀerent hardware characteristics of these de-\nvices. For example, an OpenCL code which was optimized\nfor a GPU may perform poorly on a CPU and vice versa.\nThe performance portability issues may even exist among\ndiﬀerent generations of devices developed by the same ven-\ndor [26]. Moreover, code performance may be sensitive to\ninput size, structure, or application settings, so a code op-\ntimized for some input may run sub-optimally when the\ninput is changed [20, 43].\nA costly solution to this problem is to manually op-\ntimize code for each utilized device and possibly also for\nmultiple sizes or structures of the input. An alternative so-\nlution is a technique called autotuning. Autotuning allows\noptimizing the application’s tuning parameters (properties\ninﬂuencing the application performance) in order to per-\nform the execution more eﬃciently. It is a general tech-\nnique with a broad range of applications, which includes\nareas such as network protocols, compilers, and database\nPreprint submitted to Elsevier\nMarch 2, 2020\narXiv:1910.08498v2  [cs.DC]  28 Feb 2020\n\nsystems. We focus on autotuning of code optimization pa-\nrameters, which allows changing the application at the\nlevel of its source code: from low-level optimizations such\nas loop-tiling or unrolling to more aggressive changes such\nas modiﬁcation of data layout or even using a diﬀerent\nalgorithm.\nIn this paper, we introduce the Kernel Tuning Toolkit\n(KTT), which focuses on autotuning of codes written in\nOpenCL or CUDA. With KTT, tuning parameters change\nthe source code in a way deﬁned by a programmer via\npreprocessor macros. Thus, tuning parameters may aﬀect\nvirtually any property of the source code, making auto-\ntuning very powerful. KTT targets expert programmers,\nas potential code optimizations have to be implemented\nexplicitly, requiring detailed knowledge of hardware archi-\ntectures.\nAutotuning can be performed oﬄine1, i. e., before the\nexecution of a tuned code. Oﬄine tuning is easier to imple-\nment but does not allow an application to re-tune when its\nenvironment changes. Online autotuning allows the appli-\ncation to tune itself during runtime by means of changing\nsome runtime parameters. With dynamic autotuning, the\napplication can even build the space of diﬀerent variants\nduring runtime, i. e., it is able to compile tuned kernels\nduring the tuning process. Although several code param-\neters autotuning frameworks for heterogeneous computing\nhave been introduced [3, 35, 36, 47], they are intended to\nbe used in a standalone tuning tool, supporting oﬄine au-\ntotuning only. On the other hand, KTT can be integrated\ninto application code and supports also dynamic tuning.\nA tighter integration into applications has been re-\ncently identiﬁed as one of the main challenges in auto-\ntuning [6]. Kernel Tuning Toolkit was designed to sim-\nplify the integration process. It acts as an intermediate\nlayer between the application and OpenCL or CUDA API.\nTherefore, the application source code has to be adapted to\nincorporate KTT calls. However, once integrated, the ap-\nplication can transparently switch between execution and\ntuning of the kernels. For example, the application can\nre-tune itself if it is executed on new hardware, or start its\nexecution with already optimized tuning parameters, and\nautomatically start re-tuning during runtime when the in-\nput changes.\nUsing KTT, we have developed a benchmark set com-\nprising ten autotuned codes. We have executed the bench-\nmark set on multiple hardware devices, including GPUs\nfrom NVIDIA and AMD, CPU and the Xeon Phi.\nWe\nprove that our autotuned implementations are eﬃcient\nenough – they often reach performance close to the the-\noretical peak of the hardware or at least outperform the\nbaseline (i. e., not autotuned) implementations signiﬁcantly.\nWe also show that autotuning is required to ensure per-\nformance portability of the codes.\nThe search for eﬃcient tuning conﬁgurations may be\nchallenging due to the discrete and non-linear nature of\n1We adopt the nomenclature from [6].\ntuning spaces [6]. Therefore, large tuning spaces are usu-\nally impossible to explore during application runtime. How-\never, if tuning spaces a are created rationally (i. e., by\nan expert programmer), their exploration may be feasi-\nble even at runtime. Expert programmers have to under-\nstand the eﬀect of tuning parameters and set reasonable\nboundaries to their values. For example, setting the ac-\nceptable sizes of work-groups to multiples of 32, as it is\nsuitable for vectorization on CPUs and Xeon Phis and eﬃ-\ncient on GPUs executing work-items in warps. We show in\nthis paper that rationally constructed tuning spaces can be\nmoderately-sized (thousands of conﬁgurations or less) and\nstill contain enough good conﬁgurations required for per-\nformance portability. Such tuning spaces can be searched\nduring application runtime without too high overhead. To\nprove the applicability of KTT in real applications, we\ndemonstrate dynamic autotuning in a CUDA-accelerated\n3D Fourier Reconstruction in Xmipp [43].\nThe paper makes the following major contributions:\n• Development of dynamic autotuning techniques in\nthe Kernel Tuning Toolkit. KTT introduces a high-\nlevel API for kernels and data manipulation, which\ncan be easily used and integrated into applications.\nIt allows switching transparently between autotun-\ning and executing tuned kernels. KTT is open-source2,\nfully documented, and contains many examples of its\nusage.\n• Introduction of a benchmark set of autotuned kernels.\nWe have conducted a benchmark set, including mul-\ntiple kernels relevant for HPC, spanning across mul-\ntiple application domains such as image processing,\nlinear algebra, computational chemistry, and diﬀer-\nential equations. We demonstrate that autotuning of\noptimization parameters improves the performance\nportability of the benchmark set across a range of dif-\nferent heterogeneous architectures signiﬁcantly. We\nalso show that rationally constructed tuning spaces\ncan be searched fast enough to allow dynamic tuning\nin many cases.\n• Demonstration of dynamic tuning with a real-word\napplication. We show that dynamic autotuning can\nbe used in a real-world application, such as a 3D\nFourier Reconstruction. Dynamic autotuning is also\ndemonstrated on an application performing batched\nmatrix multiplication with varying matrix sizes. We\nexperimentally evaluate the speed of tuning space\nsearch convergence as well as dynamic tuning over-\nhead on these examples.\nThe rest of the paper is organized as follows. In Sec-\ntion 2, we introduce related work and compare it with our\nwork.\nThe main design decisions and concepts of KTT\nare described in Section 3. Section 4 introduces a set of\n2https://github.com/Fillo7/KTT\n2\n\nten autotunable benchmarks and evaluates their eﬃciency\nand performance portability. Dynamic autotuning is eval-\nuated in Section 5. We conclude and sketch future work\nin Section 6.\n2. Related Work\nIn this section, we compare our work to state-of-the\nart methods in autotuning in three areas: tuning targets\n(which properties are tuned), tuning time (when tuning is\nperformed) and search strategies (how the tuning space is\nsearched and evaluated).\nAutotuning covers a broad range of empirically tuned\nparameters related to application performance, such as\ncompiler parameters, or the runtime environment [19, 18].\nSome autotuners do not required to modify the application\nsource code, for example, compiler ﬂags tuners [5] or MPI\ntuner [32]. Other tuners may change application source\ncode in order to test diﬀerent code optimization variants.\nWe focus on the autotuners altering the code of applica-\ntions in the rest of this section as they are directly related\nto our tuner.\nAutotuning is already successfully deployed in some\nhigh-performance libraries for conventional CPUs, such as\nATLAS [48] (linear algebra) or FFTW [16] (signal process-\ning). Libraries for accelerators are also often improved by\nautotuning [27, 24, 28, 31]. However, those libraries use\nautotuners specially designed for them. Here, we are in-\nterested in generic autotuners. Frameworks for skeletons\nor DSLs also use autotuning to search for the best combi-\nnation of the implementation variants empirically [12, 23,\n14, 4]. While they cover a broader range of applications\ncompared to autotuned libraries, they are still restricted\nto a particular problem domain or a set of skeletons.\nCode optimizations autotuners generate multiple func-\ntionally-equivalent variants of the application source code.\nThey may select one of the predeﬁned variants of a tuned\nfunction [34], or generate and compile implementations ac-\ncording to the values of the tuning parameters. We distin-\nguish between compiler-based tuning, where the space of\ncode transformation is generated automatically [38, 44, 40]\nand user-deﬁned code optimization parameters autotun-\ning [35, 13, 36, 15]. User-deﬁned code optimization pa-\nrameters tuning requires expert programmers to identify\nand implement tuning possibilities in the source code man-\nually (e. g., by using preprocessor macros). Even though\nthis approach may be costly in terms of time and expertise\nof the programmer, it allows to explore highly diversiﬁed\nvariants of the code, which usually cannot be generated\nautomatically by compilers: the programmer can change\nvirtually anything, for example, alter algorithms (e. g., use\nmerge sort instead of quicksort) or change the data layout\nin the memory (e. g., use a structure of arrays instead of\nan array of structures).\nOur Kernel Tuning Toolkit focuses on tuning of user-\ndeﬁned code optimization parameters. Most similar to our\nwork are CLTune [35], AUMA [13], ATF [37, 36], and Ker-\nnel Tuner [47], which are problem domain-agnostic auto-\ntuners designed for heterogeneous computing.\nExisting benchmark sets for heterogeneous computing,\nsuch as Parboil [42], SHOC [9], or Polybench/GPU [22]\ndo not support autotuning of code optimization parame-\nters (only work-group size can be typically changed with-\nout substantial rewriting the benchmark). To the best of\nour knowledge, there is no comprehensive benchmark set\nfor code optimization parameters tuning in heterogeneous\ncomputing. In [35, 36], two benchmarks are used to eval-\nuate code optimization parameters tuning: GEMM and\n2D convolutions. Those benchmarks are also used in our\nbenchmark set. Three benchmarks are used in [47] (one of\nthem is the GEMM introduced in [35]) and in [13]. In our\nwork, a set of ten benchmarks is introduced.\nSome forms of dynamic autotuning are supported by\nproblem-speciﬁc autotuners, such as SpMV tuning [20] or\ngeneric autotuners, such as Active Harmony [44]. Auto-\ntuners may also support online autotuning where usually\nmultiple variants of code are produced in an oﬄine phase\nand searched during runtime. Online tuning is easier to\nimplement than dynamic tuning (there is no runtime com-\npilation), but it is not practical when the number of possi-\nble code variants is high. An examples of an online tuner\nis SOCRATES [17, 33].\nNone of the frameworks for code optimizations in het-\nerogeneous computing support dynamic tuning natively [35,\n13, 36, 47].\nTo implement dynamic tuning with those\nframeworks, the programmer has to add a non-trivial amount\nof glue code, running the tuner during application run-\ntime to ﬁnd a better tuning conﬁguration and then ex-\nporting this conﬁguration into the application, typically\nby re-compiling OpenCL or CUDA kernels with the JIT\ncompiler. The OpenTuner [3], another similar tuner, is a\nmore generic and low-level tool: it allows us to tune virtu-\nally any property of the application, but a higher amount\nof user eﬀort has to be invested into the integration of the\ntuner. OpenTuner could be used for dynamic autotuning\nwith higher eﬀort than [35, 13, 36, 47], since a code re-\nsponsible for a tuned kernel compilation, execution, and\ntesting has to be provided as well. On the other hand,\nOpenTuner would allow to use results computed by ker-\nnels during tuning, which can increase the performance of\nthe tuned application. To the best of our knowledge, KTT\nis the ﬁrst autotuning framework combining universal code\noptimization parameters tuning with native support of dy-\nnamic autotuning for heterogeneous computing.\nIn this paper, we extend state-of-the-art general-purpose\ncode optimizations autotuners for heterogeneous comput-\ning with dynamic tuning.\nAlthough the concept of dy-\nnamic autotuning is well-known, it requires an architecture\nthat hides the OpenCL or CUDA API in order to switch\nimplementation of kernels. In addition we contribute to\nthe state-of-the-art in autotuning by introducing a bench-\nmark set of autotunable codes, evaluating the eﬃciency\nand performance portability of the benchmarks, and as-\n3\n\nsessing how diﬃcult it is to amortize the overheads of dy-\nnamic tuning.\nThe space of tuning parameters can be very diﬃcult\nto search: it is discrete, non-linear, and non-convex. Al-\nthough a promising method has been recently published [47],\nthe majority of papers report that random search is often\nas eﬃcient or even more eﬃcient than more sophisticated\nsearch methods [25, 39, 35]. Therefore, it can be diﬃcult\nto search large tuning spaces containing hundreds of thou-\nsands of conﬁgurations or more. Extremely large tuning\nspaces, however, result mainly from compiler-based au-\ntotuners such as Lift [40] or naively constructed tuning\nspaces. The papers [25, 39, 35, 47] focus on the analysis\nof tuning space search methods. However, there has not\nbeen much eﬀort invested into studying the size of tuning\nspaces using a larger number of benchmarks which main-\ntain good performance portability across a wide range of\ndiﬀerent hardware architectures. In this paper, we con-\nstructed a set of ten benchmarks and show that tuning\nspaces are often small enough to be searched dynamically\nwhile still providing performance portability with a near-\npeak performance.\nMachine learning on historical autotuning data can be\nused to decrease the number of tuning decisions performed\nduring program compilation or execution. In [33], a dy-\nnamic selection from a very limited number of code vari-\nants is based on a model created from previous tuning\nruns. In [8], a single tuning parameter can be optimized\nat compilation time by a neural network trained in mul-\ntiple trial runs.\nContrary to those papers, we focus on\nmulti-dimensional tuning spaces.\n3. Architecture of the Kernel Tuning Toolkit\nIn this section, we introduce the main architectural\nconcepts and the API of the Kernel Tuning Toolkit. We\nare using the following terminology in the paper. A tun-\ning parameter is a variable which aﬀects the code in a\nuser-deﬁned way (e. g., determines loop unroll factor). The\ntuning space is a cross product of all the possible values\nof all tuning parameters. A conﬁguration is a single point\nin the tuning space (i. e., assignment of concrete values to\nall tuning parameters), which fully determines one possi-\nble implementation variant of the tuned code. The main\nfunctionality of KTT is:\n• speciﬁcation of tuning parameters and constraints of\ntuning space;\n• compiling and executing the kernel or kernel compo-\nsition (multiple kernels and host code with shared\ntuning parameters);\n• automatically searching the tuning space;\n• managing data transfers automatically (KTT auto-\nmatically creates and copies data from/to the accel-\nerator);\nFigure 1: Schematic view of KTT architecture.\nThe dashed line\nshows components, which are typically active during dynamic tuning\ninside the main application loop.\n• checking the results of the tuned kernel against a\nreference implementation computation.\nKTT has been designed as a C++ library, which re-\nplaces direct access to the OpenCL or CUDA API. By\nproviding a middle layer between the application and the\nOpenCL or CUDA API, KTT is able to perform autotun-\ning transparently: the kernel execution and tuning can\nbe performed by the same application code.\nHowever,\nin order to allow integration into real-world applications,\nKTT must support important functionality such as mem-\nory management, kernel conﬁguration, execution, and syn-\nchronization provided by OpenCL or CUDA. Because KTT\nforms a middle layer between the application and the CUDA\nor OpenCL API, it can modify kernel code at runtime,\ntransparently to the application. Moreover, this design al-\nlows switching between OpenCL and CUDA easily. When\nkernel codes for both APIs are provided by the program-\nmer, the KTT is just initialized with the selected API and\nhandles all the communication between the application\nand OpenCL or CUDA. Because OpenCL and CUDA use\na diﬀerent way to conﬁgure the parallelism of the kernel3,\nKTT can automatically translate parallelism conﬁguration\nfor the selected API. The KTT API has been derived from\nthe CLTune project [35], so it is very similar to CLTune\nwhen we use it for oﬄine tuning. Additionally, KTT API\nallows for tuning compositions of multiple kernels, tuning\nof how kernels are called from host code [15], and novel\nfeatures for dynamic tuning.\n3OpenCL’s\nNDRange\ndescribes\nglobal\nparallelism,\nwhereas\nCUDA blocks and threads deﬁne diﬀerent layers of parallelism\n4\n\nThe architecture of KTT and its connection to the au-\ntotuned application is sketched in Figure 1. The appli-\ncation creates kernels and deﬁnes tuning parameters and\ntheir acceptable values (with possible constraints passed\nas lambda functions), and passes them to KTT, where\nthe tuning space is built.\nThen, it connects input and\noutput buﬀers to the kernels and starts the tuning pro-\ncess.\nKTT uses a searcher to search the tuning space\nand to select a conﬁguration to be executed. In current\nimplementation, random search, simulated annealing and\nMarkov-chain Monte Carlo searchers are available. Then,\nit compiles the kernel(s) according to the selected conﬁgu-\nration, executes and benchmarks it. If dynamic tuning is\nactive, the results of the tuned kernel(s) can be immedi-\nately used by the application. The results can be validated\nagainst a reference implementation by KTT. The execu-\ntion of kernel(s) is benchmarked and the performance re-\nsults are stored in KTT, allowing the searcher to navigate\nthe search process and the application to query for, e. g.,\nthe fastest conﬁguration.\n3.1. Kernel tuning\nThe simplest scenario is tuning of a single kernel. In\nthis case, the following steps have to be done in a tunable\ncode:\n• initialize the tuner;\n• create handlers for kernel arguments;\n• create the kernel handler;\n• assign input/output arguments to the kernel;\n• deﬁne tuning parameters, their acceptable values,\nand constraints;\n• start tuning.\nThe tuner executes and benchmarks diﬀerent tuning con-\nﬁgurations and searches for the one, which results in the\nshortest kernel runtime.\nIn many real-world applications, some tuning parame-\nters are shared between multiple kernels (e. g., the memory\nlayout of some intermediate data). KTT framework allows\nsharing tuning parameters among kernels by using ker-\nnel compositions. Moreover, a portion of a computation\ncan be performed on the host (e. g., a tuning parameter\nmay determine how many times a kernel is executed or if\na host code performs some pre-computation). KTT uses\nthe tuning manipulator when tuning parameters inﬂuence\nthe host code. The tuning manipulator class enables users\nto customize a portion of the framework’s code that is\nresponsible for kernel execution and buﬀer management,\nand optionally can perform some part of the computation\ndirectly in the C++ host code. The tuning manipulator\nmust implement a method launchComputation, which can\nexecute multiple kernels, perform computations in C++,\nand transfer data between host and device. Tuning ma-\nnipulators and kernel compositions allow to use tuning pa-\nrameters, which cannot be implemented when kernels are\ntuned separately: for example, it is possible to change a\nformat of the intermediate data exchanged between mul-\ntiple kernels.\n3.2. Oﬄine and Dynamic tuning\nKTT supports diﬀerent types of autotuning depending\non the time when tuning is performed and on the level of\nintegration:\n• Oﬄine autotuning is performed prior to the execu-\ntion of an application, usually by an extra utility.\nOﬄine tuning does not require integration of the au-\ntotuner into the application. The tuning utility can\nsearch for tuning parameters of the computationally\nmost demanding application kernels and then ex-\nports values of those parameters to the build system.\nThe disadvantage is that the tuning process cannot\nbe easily repeated inside the application, i. e., during\napplication runtime.\n• Dynamic autotuning is performed during applica-\ntion runtime. When tuning parameters change ap-\nplication source code, it must be modiﬁed accord-\ning to the actual values of the tuning parameters\nand recompiled.\nThe application can execute au-\ntotuning at any time, e. g., when it is executed on a\nnew hardware device or when a performance-relevant\ncharacteristic of the processed data changes4. Dy-\nnamic tuning can be performed in a blocking man-\nner (the tuner tests several tuning conﬁgurations and\nselects the best one; the results of kernel executions\nare not passed to the application during tuning) or\nnon-blocking manner (the result of each tested ker-\nnel variant is immediately used by the application).\nWith blocking autotuning, KTT automatically repli-\ncates input and output arrays, so there is no side ef-\nfect caused by kernel results on the application. Non-\nblocking tuning is more suitable for interactive ap-\nplications or complex parallel workloads with many\ndependent tasks, where a slow response of some com-\nponent may be critical to the overall performance.\nThe Kernel Tuning Toolkit can be integrated into ap-\nplication code so that the application code manages mem-\nory objects and executes kernels via the KTT API instead\nof directly using OpenCL or CUDA. In such a case, the\napplication decides if KTT changes values of the tuning\nparameters and recompiles kernels (tuning mode) or if\nKTT just executes the kernels (running mode). Further-\nmore, the kernels’ result can be used by the application\neven during the tuning process (a non-blocking tuning de-\nscribed above), which improves application performance,\n4With the current version of KTT, the decision about when to\ntune is always made by the application.\n5\n\nespecially when the tuning overhead is low (i. e. kernels\nruntime dominates compilation runtime).\n3.2.1. Code Example\nLet us assume we have two kernels, foo(a) and bar(b).\nThe kernel foo produces a 2D array, which is used as an\ninput for kernel bar: b = foo(a); c = bar(b);. Let us\nfurther assume a tuning parameter B TRANS, which de-\ntermines if b is stored transposed. Clearly, the value of\nB TRANS must be the same for both foo and bar, so the\nkernels must be tuned together. Thus, we create a kernel\ncomposition with a tuning manipulator calling both ker-\nnels. The tuning manipulator is shown in Listing 1. The\nclass inherited from tuning manipulator must override the\nmethod launchComputation, which is responsible for ex-\necuting the two kernels via KTT in our example, but it\ncould also implement computation in C++ or call KTT\nfunctions for data movement or synchronization.\n1\nclass\nTunableFoobar\n:\npublic\nktt : : TuningManipulator\n{\n2\npublic :\n3\nTunableFoobar ( ktt : : KernelId\nfoo ,\n. . .\n)\n:\n4\n//\nassign\nkernels\nand\ninput / output\n5\n//\nto\ni n t e r n a l\ns t r u c t u r e s\n6\n{}\n7\nvoid\nlaunchComputation ( const\nktt : : KernelId )\n8\no v e r r i d e\n{\n9\n//\ntuning\nparameters\ncan\nbe\nqueried\nhere\n10\nrunKernel ( foo ) ;\n11\nrunKernel ( bar ) ;\n12\n}\n13\nprivate :\n14\nktt : : KernelId\nfoo ;\n15\n. . .\n16\n} ;\nListing 1: Tuning manipulator\nThe code setting up KTT is sketched in Listing 2. It\ninitializes the tuner at line 2, creates kernels (lines 5-8),\ntheir arguments (lines 11-12), and constructs a composi-\ntion of the kernels (lines 16-22). The composition is cre-\nated with a tuning manipulator implemented in a class\nTunableFoobar (see Listing 1). The kernels are created\nwith an initial conﬁguration of NDRange and work-group\nsize (lines 3-4), but this conﬁguration can be altered in\ntwo ways: by deﬁning the relation of NDRange/group size\nand some tuning parameter (using a pre-deﬁned or lambda\nfunction), or directly in launchComputation method by\nany user code.\n1\n//\nI n i t i a l i z e\ntuner\nand\nkernels\nfoo ,\nbar\n2\nktt : : Tuner\ntuner ( platformIndex ,\ndeviceIndex ) ;\n3\nconst\nktt : : DimensionVector\nndRange ( i n p u t S i z e ) ;\n4\nconst\nktt : : DimensionVector\nworkGroup ( 1 2 8 ) ;\n5\nktt : : KernelId\nfoo = tuner . addKernelFromFile (\n6\nk e r n e l F i l e ,\n” foo ” ,\nndRange ,\nworkGroup ) ;\n7\nktt : : KernelId\nbar = tuner . addKernelFromFile (\n8\nk e r n e l F i l e ,\n” bar ” ,\nndRange ,\nworkGroup ) ;\n9\n10\n//\nCreation\nof\nkernel\narguments a ,\nb ,\nc\n11\nktt : : ArgumentId\na = tuner−>addArgumentVector ( srcA ,\n12\nktt : : ArgumentAccessType : : ReadOnly ) ;\n13\n. . .\n14\n15\n//\nCreation\nof\ncomposition\nand\ns e t t i n g\nof\narguments\n16\nktt : : KernelId\ncompositionId = tuner . addComposition (\n17\n” foobar ” ,\nstd : : vector<ktt : : KernelId >{foo ,\nbar } ,\n18\nstd : : make unique<TunableFoobar >(foo ,\nbar ,\na ,\nb ,\nc ) ) ;\n19\ntuner . setCompositionKernelArguments ( compositionId ,\n20\nfoo ,\nstd : : vector <s i z e t >{a ,\nb } ) ;\n21\ntuner . setCompositionKernelArguments ( compositionId ,\n22\nbar ,\nstd : : vector <s i z e t >{b ,\nc } ) ;\n23\n24\n//\nAddition\nof\ntuning\nv a r i a b l e s\n25\ntuner . addParameter ( compositionId ,\n”B TRANS” ,\n{0 ,\n1 } ) ;\nListing 2: Tuning initialization\nAfter the setup, we can perform kernel tuning. Here,\nwe demonstrate non-blocking dynamic autotuning, which\nis performed in the main application loop as sketched in\nListing 3.\nIn our simple example, we use the variable\ntuningOn to specify whether dynamic tuning is performed\n(it can be set by some user-deﬁned function to true for a\nﬁxed number of iterations, or until some predeﬁned per-\nformance is reached).\nThe execution of a composition\ncalling foo and bar can be achieved by two methods:\nrunKernel or tuneKernelByStep.\nThe runKernel exe-\ncutes the composition and stores result in variable c. The\nexecution is performed with a tuning conﬁguration deﬁned\nby the programmer; usually, the fastest conﬁguration is\nused. The second method, tuneKernelByStep, also per-\nforms the computation and stores results in c, but with a\nnew values of the tuning parameters (selected by KTT us-\ning the selected search method). If the tuning space has al-\nready been explored, the method tuneKernelByStep exe-\ncutes the conﬁguration, which results in the fastest compu-\ntation (so it behaves like runKernel executed with the best\nconﬁguration). If the application is exploring only a subset\nof the tuning space, it can query the fastest known conﬁg-\nuration via the getBestComputationResult method. The\nrest of the application does not need to be aware whether\ntuning is performed: the result c is obtained in any case.\n1\nwhile ( a p p l i c a t i o n r u n )\n{\n2\n. .\n3\ni f\n( tuningOn )\n4\ntuner . tuneKernelByStep ( compositionId ,\n{c } ) ;\n5\nelse\n{\n6\nktt : : ComputationResult\nbest =\n7\ntuner−>getBestComputationResult ( compositionId ) ;\n8\ntuner . runKernel ( compositionId ,\n9\nbest . getConfiguration ( ) ,\n{c } ) ;\n10\n}\n11\n//\nc\ni s\ncomputed\nhere\n12\n. . .\n13\n}\nListing 3: Main loop performing computation\n3.3. Independent Queues and Non-blocking Calls\nAccelerated codes often employ task-level parallelism\nto overlap computation on a host, computation on a de-\nvice, and data movements between the host and the device.\nMoreover, simultaneous kernel execution may improve the\nperformance of independent kernels when some kernels do\nnot fully utilize the device. Task-level parallelism is real-\nized via non-blocking kernel calls, asynchronous copy and\nalso via multiple queues (OpenCL) or streams (CUDA).\nIn order to reach high performance when integrated\ninto an application, KTT must support this functionality\nfor the tuned kernels. Thus, it is possible to use queues\n(when using CUDA, KTT queues are implemented as CUDA\nstreams) and non-blocking calls with KTT. However, dur-\ning the tuning of the kernel, concurrent kernel execution\n6\n\nor non-blocking execution may bias benchmarking (e. g.,\nwith concurrent kernel execution, the host code can exe-\ncute another kernel at the device where the tuned kernel is\nrunning, so the measuted runtime of the tuned kernel in-\ncreases). The bias in benchmarking could result in a wrong\nselection of the best tuning parameter values. Therefore,\nthere are two types of task-level parallelism implemented\nin KTT:\n• intra-manipulator parallelism allows simultaneous ker-\nnel execution and overlapping computations and mem-\nory copy inside a launchComputation method of a\ntuning manipulator;\n• global parallelism also allows simultaneous kernel ex-\necution and non-blocking kernel calls at the level\nof the application code, so the host code can call\nrunKernel in non-blocking mode, allowing to over-\nlap execution of multiple manipulators, or host and\ndevice computation.\nDuring the tuning process, global parallelism is not al-\nlowed, so only one tuning conﬁguration is executed at a\ntime. Therefore, benchmarking is not biased by executing\nanother code on a computing device or in a CPU thread\nwhere KTT is running.\nHowever, tuning manipulators\nmay still use intra-manipulator parallelism, so it is still\npossible to, e. g., execute multiple independent kernels in\nparallel, or overlap kernel execution with the data copy or\nthe CPU code.\nWhen the tuning process ends, KTT also allows the\nglobal parallelism so that kernels or composition calls can\nbe overlapped with another device or host code. Note that\nthe result of the kernel or composition is downloaded to the\nhost memory by default, which enforces synchronization.\nHowever, the user can create persistent arguments, which\nare not copied to the host by KTT unless the application\nexplicitly calls the proper KTT copy method.\n3.4. Limitations\nRecall that KTT forms an intermediate layer between a\ntuned application and the OpenCL or CUDA API. There-\nfore, it has to implement the interface to operate those\nAPIs. The current implementation of KTT does not sup-\nport all the features of CUDA and OpenCL. Due to the\nlack of OpenCL 2.0 implementation for NVIDIA GPUs,\nthe OpenCL support is limited to OpenCL 1.2 with KTT.\nAlso, some features of CUDA are not supported: texture,\nsurface, and constant memory and cooperative grids. We\nbelieve that there is no fundamental problem to support\nthose features in a future version of KTT.\nThe new features of CUDA and OpenCL, which require\nchanges in kernel code only, do not require any explicit\nsupport in KTT, as KTT methods replace only the host\nAPI (for example, new warp-level synchronization or warp-\nmatrix operations executed on new CUDA tensor cores can\nbe used with KTT without any explicit support).\nIn its current implementation, a single instance of KTT\nworks with a single computing device. To use multiple de-\nvices (e. g., in multi-GPU machine), the programmer has\nto create multiple instances of KTT and partition the tun-\ning space manually. It also implies that there is no explicit\nsupport for tuning which device is to be used for which\nparticular kernel.\n4. Autotuning Benchmarks\nIn this section, we introduce a set of ten tunable bench-\nmarks. Each benchmark contains a C++ code, which pre-\npares data and performs tuning with KTT, and OpenCL\nor CUDA code of tunable kernels. We brieﬂy introduce\ntheir implementation and evaluate the beneﬁts of auto-\ntuning by measuring their eﬃciency and assessing their\nperformance portability. All benchmarks have been tuned\nfor and evaluated on seven diﬀerent hardware devices as\nlisted in Table 1.\nDevice\nArchitecture\nSP perf.\nBW\n2× Xeon E5-2650\nSandy Bridge\n512\n102\nXeon Phi 5110P\nKnights Corner\n2,022\n320\nTesla K20\nKepler\n3,524\n208\nGeForce GTX 750\nMaxwell\n1,044\n80\nGeForce GTX 1070\nPascal\n5,783\n256\nRadeon RX Vega 56\nGCN 5\n8,286\n410\nGeForce RTX 2080Ti\nTuring\n11,750\n616\nTable 1: Devices used in our benchmarks. Arithmetic performance\n(SP perf.)\nis measured in single-precision GFlops, memory band-\nwidth (BW) is measured in GB/s.\n4.1. Tuning Parameters\nWith tuning of code optimization parameters, the tun-\ning parameters can encode virtually any change of the\nsource code. While many benchmarks contain tuning pa-\nrameters performing the same type of optimization, their\nimplementation may diﬀer from case to case. In this sec-\ntion, we describe the common optimizations parameters\nimplemented in most of the benchmarks.\n4.1.1. Work-group Size\nOn GPUs, the size of work-group allows balancing the\namount of reachable parallelism (i. e., amount of work-\nitems which can run simultaneously) and allocated re-\nsources (e. g., private and local memory consumption). In\ngeneral, smaller work-groups (to some extent) allow to al-\nlocate of more resources and reduce local barrier overhead.\nOn the other hand, small work-groups may decrease mem-\nory locality when some type of memory blocking is used.\nVery small work-groups may also decrease reachable paral-\nlelism due to creation of under-populated warps or due to\nthe limited amount of work-groups which can be placed on\nGPU simultaneously. On CPUs, work-items are processed\nin a vectorized loop and thus the work-group size mainly\n7\n\ninﬂuences the amount of consumed registers and memory\nlocality.\nThe optimization of work-group size (or block size in\nCUDA) is a common optimization method, which may be\neasily implemented without re-compilation of the kernel\ncode. However, most of the integer arithmetic required for\narray indexing uses the work-group size.\nConsequently,\nwhen the work-group size is encoded by a tuning parame-\nter, indexing arithmetics can be optimized during compi-\nlation.\n4.1.2. Work-item Coarsening\nWork-item coarsening (or thread coarsening in CUDA)\nis a well-known technique [45, 41], optimizing the amount\nof work per work-item. On GPUs, adding more work per\nwork-item improves private memory locality and instruction-\nlevel parallelism. On the other hand, it also increases the\nnumber of used registers, so that the reachable parallelism\ncan be reduced. Work-item coarsening is similar to the\nloop unrolling on CPUs, as each work-item (i. e., iteration\nof the generated vectorized loop) performs more computa-\ntions.\n4.1.3. Caching in Local Memory\nLocal memory (called shared memory in CUDA) is\nGPU-speciﬁc hardware, which allows work-items from the\nsame work-group to share data. It is often used as an ex-\nplicit cache, where data loaded from global memory are\nfurther processed (or where data are collected before they\nare moved to the global memory). Local memory is faster\nthan global memory and usually also faster than global\nmemory cache. On the other hand, explicit caching may\nbe challenging with more complex memory access patterns.\nTherefore, it may or may not be eﬃcient to cache data in\nlocal memory.\nOn CPUs, there is no special hardware for local mem-\nory – data allocated in the local memory are placed in a\nbuﬀer in the global memory. Therefore, there is no reason\nto use it for improving the speed of the code, but it can\nbe still used to share data between work-items.\n4.1.4. Caching in Private Memory\nPrivate memory (or registers in CUDA) is the fastest\nmemory available for both GPUs and CPUs.\nExplicit\ncaching in private memory speeds-up access to the data.\nHowever, it may also lead to registers spilling on both GPU\nand CPU architectures.\n4.1.5. Tile Size\nMemory tiling is a common technique to improve spa-\ntial or temporal locality. It is usable for direct global mem-\nory access (a tile is stored in the cache by the hardware),\nor explicit caching in local or private memory. The tile\nsize may or may not be equal to the work-group size (e. g.,\nwork-items can process multiple data elements, so the tile\nsize is an integer multiple of work-group size). Bigger tiles\nensure better cache locality as long as cache capacity is\nnot exceeded. However, with explicit caching on GPUs,\nbigger tiles can reduce reachable parallelism by increasing\nresources consumption.\n4.1.6. Loop Unrolling\nLoop unrolling is a general technique, which allows in-\ncreasing instruction-level parallelism, reducing branching\nand simplifying array indexing by common subexpression\nelimination. It increases the performance of loops if there\nare enough registers available.\n4.1.7. Padding Local Memory\nGPU local memory consists of multiple banks (usually\n32), which should be accessed in parallel to reach the high-\nest performance. If diﬀerent data from the same bank are\nread, a bank conﬂict occurs and the access into this bank is\nserialized, resulting in performance degradation. Padding\narrays in local memory can prevent bank conﬂicts in some\nsituations.\nFor example, parallel read of a column of a\n32 × 32 matrix in local memory results in a 32-way bank\nconﬂict. However, when the matrix is stored as 33 × 32\narray, there is no conﬂict in accessing columns.\n4.1.8. Explicit Vectorization\nThe code performed by work-items can be written in\na vectorized form.\nSuch a case is similar to loop un-\nrolling with slightly modiﬁed eﬀect. With GPUs, it is eas-\nier for the compiler to generate faster vector instructions\nfor memory access (both global and local). With CPUs,\nthe OpenCL compiler by default performs de-vectorization\nand vectorization, but it can be hinted to directly translate\nvectorized code into vector instructions, which can help if\nimplicit vectorization is not eﬃcient enough. On the other\nhand, explicit vectorization often increases register usage\non GPUs. It may also increase the amount of workload\nper work-group, which increases registers pressure in case\nlocal barriers are called within the kernel.\n4.2. Benchmark Set Implementation\nHere, we introduce the implementation of the bench-\nmark set used in this paper. As the development of au-\ntotuning benchmarks is quite a time consuming task (the\ntuning parameters have to be identiﬁed in the code, and\ntheir eﬀect has to be implemented), we have composed a\nbenchmark set from already available kernels, kernels de-\nveloped by our group in several projects, and kernels de-\nveloped as autotuned variants of previously available non-\nautotuned kernels. The benchmarks set covers important\ncomputational problems spanning across multiple applica-\ntion domains: image processing (3D Fourier Reconstruc-\ntion and 2D Convolution), linear algebra (BiCG, GEMM,\nGEMM Batched, Matrix transpose, and Reduction), com-\nputational chemistry (Direct Coulomb Summation) and\ndiﬀerential equation solvers (N-body and Hotspot). Most\nof the benchmarks use tuning parameters for performing\n8\n\nthe optimizations introduced in Section 4.1. Table 2 shows\nwhich optimizations are implemented by which particu-\nlar benchmark. Benchmarks which have been published\npreviously are described brieﬂy here, whereas the unpub-\nlished benchmarks are introduced in greater detail. Multi-\nple benchmarks also implement special optimizations not\nlisted in the table – in such case, the optimizations are\nmentioned in the benchmark description in this section.\nThe benchmark set is publicly available. Except for\n3D Fourier Reconstruction, all benchmarks are bundled\nwith the Kernel Tuning Toolkit as examples of its usage5.\nThe autotuned version of 3D Fourier Reconstruction is cur-\nrently not integrated into the production version of Xmipp,\nbut it can be downloaded from Github6.\n4.2.1. BiCG\nBiCG is a kernel used in the biconjugate gradient method.\nIt computes\nq = Ap\ns = AT r\n(1)\nwhere A is a matrix and p, q, r, s are vectors.\nWe have\nadopted the implementation from PolyBench/GPU [22]\nand implemented kernel fusion and cache tiling similarly\nto our previous work [14].\nIn addition to the parame-\nters listed in Table 2, we have created tuning parameters\nchanging the following properties of the code:\n• whether BiCG is computed by the fused kernel (load-\ning matrix A only once), or by two separate kernels\ncomputing Ap and AT r;\n• the amount of work per work-group (it can iter-\nate over multiple tiles, improving memory locality\nof output vectors);\n• how the reduction of resulting vectors is performed\n(can be reduced in local memory or global memory);\n• how reduction is implemented (using atomic opera-\ntions, or ﬁnishing reduction in a separate kernel).\nThe implementation uses the tuning manipulator, as tun-\ning parameters change the execution of kernels (e. g., when\natomics are not used, an extra kernel is needed to ﬁnish\ncomputation of vectors q, s).\n4.2.2. 2D Convolution\nThe 2D convolution example using 7×7 ﬁlter is adopted7\nfrom the CLTune project [35]. The special tuning param-\neters determine the way of handling shared boundaries of\ntiles.\n5https://github.com/Fillo7/KTT/examples\n6https://github.com/I2PC/scipion/tree/jd_\nreconstructFourier_KTT\n7Our code uses the same kernel and tuning space, but the appli-\ncation is modiﬁed to use KTT API.\n4.2.3. Direct Coulomb Summation\nThe direct Coulomb summation precomputes the 3D\nspatial grid of electric charge around a molecule, used,\ne. g., in molecular docking [21]. We have introduced the\nautotuned implementation in [15]. Here, we evaluate a 3D\nversion of the published algorithm. The algorithm tunes,\nbesides those mentioned in Table 2, the following param-\neters:\n• whether input atoms are stored in global or in con-\nstant memory;\n• whether input atoms are stored as a structure of ar-\nrays or as an array of structures.\n4.2.4. GEMM\nThe generalized matrix-matrix multiply (GEMM) is a\nstandard part of BLAS [10].\nIts performance is critical\nfor many applications. We have adopted an example from\nthe CLTune project [35] with a complex tuning space con-\ntaining 241,600 conﬁgurations. The large tuning space is\nmainly caused by applying optimizations listed in Table 2\nin multiple dimensions. Moreover, tuning parameters are\nprovided for switching between continuous and strided ac-\ncess to the input matrices.\n4.2.5. GEMM Batched\nRegular BLAS implementations are optimized for large\ndata vectors and matrices. However, some applications,\nsuch as deep learning [1], multifrontal solvers for sparse\nlinear systems [11] or Finite Elements Method [29] require\nexecuting many instances of BLAS routines operating on\nvery small matrices. Therefore, batched operations (i. e.,\ngrouping many BLAS calls that process small matrices\ntogether into a single call) are being developed to exploit\ncontemporary highly-parallel hardware.\nIt has been shown that autotuning enables reaching\nnear-peak performance for batched GEMM using very small\nmatrices (up to 32 × 32 elements) [30]. The implemen-\ntation of batched GEMM has to be altered for diﬀerent\nsizes of matrices [30]. We have implemented the batched\nGEMM kernel from scratch. It is optimized for very small\nmatrices similarly to [30], but also for highly rectangular\nsmall matrices. Note that for small matrices, GEMM is\nmemory-bound (it does not expose high ﬂop-to-word ra-\ntio). Therefore, optimization strategies are diﬀerent than\nfor GEMM optimized for larger matrices, resulting in a sig-\nniﬁcantly smaller tuning space. Since our GEMM Batched\nbenchmark is optimized for small matrices only, for bigger\nmatrices, the original GEMM benchmark should be used.\nOur implementation uses highly-conﬁgurable parallelism.\nFor output matrix of size m × n, a work-group of size\nm×y×z is created, where y, z are tuning parameters. Pa-\nrameter y deﬁnes work-item coarsening: it determines the\nnumber of work-items in the y-dimension which process\none instance of matrix multiplication and hence the num-\nber of elements processed by each work-item. Parameter z\n9\n\nBenchmark\nWG size\ncoarsening\nLM caching\nPM caching\nTile size\nunrolling\nLM padding\nvectorization\nBiCG\n✓\n✓\n✓\n✓\n✓\n2D Convolution\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\nCoulomb 3D\n✓\n✓\n✓\n✓\nGEMM\n✓\n✓\n✓\n✓\n✓\n✓\nGEMM Batched\n✓\n✓\n✓\n✓\n✓\n✓\n✓\nHotspot\n✓\n✓\n✓\n✓\n✓\nMatrix Transpose\n✓\n✓\n✓\n✓\n✓\n✓\nN-body\n✓\n✓\n✓\n✓\n✓\n✓\nReduction\n✓\n✓\n✓\nFourier\n✓\n✓\n✓\n✓\nTable 2: Common optimizations tuned by benchmarks. Tile size is marked when it can be conﬁgured diﬀerently than work-group size. The\nabbreviations used in the names of the columns are as follows: ”WG” is work-group, ”LM” is local memory, ”PM” is private memory.\ndetermines the number of matrix multiplication instances\ncomputed by a work-group. Caching in local memory is\nalso implemented for the output matrix: it can be written\ninto global memory directly, or multiple matrices can be\narranged in local memory and written together (improves\nmemory coalescing).\n4.2.6. Hotspot\nThe Hotspot kernel, used for calculating a heat dis-\ntribution on a 2D surface, is based on a kernel from the\nRodinia benchmark suite [7]. It implements a 2D ﬁnite dif-\nferences method, which can exploit temporal locality (as it\nis executed iteratively). We have implemented tuning pa-\nrameters listed in Table 2, and parameter allowing to tune\nthe number of steps performed in a kernel call (balances\ntemporal locality against redundant computation).\n4.2.7. Matrix Transpose\nWe have implemented autotuning for a tiled matrix\ntransposition sample from NVIDIA CUDA SDK 10.0. The\ntuning parameters additional to those deﬁned in Table 2\nare as follows:\n• transposition of work-items (work-items in a warp\nmay read rows and store columns, or read columns\nand store rows);\n• explicit prefetching into the cache.\n4.2.8. N-body\nThe computation of gravitational forces between n bod-\nies in space is based on the code sample from NVIDIA\nCUDA SDK 9.0. It computes a gravitational force between\nall pairs of bodies, and thus is a very compute-intensive\nbenchmark. We have added the tuning parameters allow-\ning tuning of how input bodies are stored (array of struc-\nture or structure of arrays) and also the optimizations de-\nﬁned in Table 2.\n4.2.9. Reduction\nThe reduction benchmark computes the sum of all el-\nements in an input vector. We have used the autotuned\nimplementation from our previous work [15]. There are\ntwo special optimizations aﬀected by tuning parameters\nand not listed in Table 2:\n• whether the reduction is performed with at most one\nglobal barrier only by a ﬁxed number of work-items,\nor iteratively by multiple kernels scaling with the size\nof the reduced vector;\n• whether, with the ﬁxed number of work-items, the\nﬁnal reduction is performed by extra kernel invoca-\ntion, or by utilizing atomic operations.\n4.2.10. 3D Fourier Reconstruction\nOne of the computationally demanding steps in the im-\nage reconstruction pipeline in cryo-electron microscopy is\na 3D Fourier reconstruction [2]: the process when 2D sam-\nples of arbitrary orientation are inserted into the 3D vol-\nume. We have used the autotuned implementation intro-\nduced in our previous work [43]. This implementation can\nbe tuned for speciﬁc hardware and also for speciﬁc samples\nresolution. In contrast to other benchmarks, 3D Fourier\nReconstruction is implemented in CUDA and therefore can\nbe evaluated on NVIDIA GPUs only.\nThe tuning space allows several optimizations not listed\nin Table 2:\n• atomic writing into output volume (allows to process\nmultiple 2D samples in parallel);\n• precomputation or on-the-ﬂy computation of inter-\npolation weights;\n• how are work-items mapped to data inside tiles of\ninput 2D samples (optimizing cache locality).\n4.2.11. Summary\nOur benchmarks use a variety of tuning parameters,\nsome of them common for multiple benchmarks, some of\nthem speciﬁc for a given computational problem.\nThe\nsize and dimensionality of tuning spaces are summarized\nin Table 3. Note that the number of tuning parameters\ncan be higher than the number of tuned optimizations\ndescribed in this section, because some optimizations are\n10\n\nBenchmark\ndimensions\nconﬁgurations\nBiCG\n11\n5,122\nConvolution\n10\n5,248\nCoulomb 3D\n8\n1,260\nGEMM\n15\n241,600\nGEMM batched\n11\n424\nHotspot\n6\n480\nTranspose\n9\n10,752\nN-body\n8\n9,408\nReduction\n5\n175\nFourier\n6\n360\nTable 3: A list of the benchmarks and the size and dimensionality\n(i. e., the number of tuning parameters) of their tuning spaces.\nBenchmark\nbound.\nops.\nnote\nBiCG\nmem\n4a2\na: width and height\nof the input matrix\nCoulomb 3D\ncomp\n6ak3\na: number of atoms,\nk:\nnumber of grid\npoints per dimension\nGEMM\ncomp\n2a3\na: width and height\nof all matrices\nGEMM batched\nmem\n12na2\na: width and height\nof\nall\nmatrices,\nn:\nnumber of matrices\nHotspot\nmem\n4ia2\na: width and height\nof the input matrix,\ni:\nnumber of itera-\ntions\nTranspose\nmem\n8a2\na: width and height\nof all matrices\nN-body\ncomp\n20n2\nn: number of bodies\nReduction\nmem\n4n\nn: size of input vec-\ntor\nTable 4: Number of operations performed by diﬀerent benchmarks.\nThe column ”bound.” distinguishes between memory-bound codes\n(operations in ”ops.” column refer to transferred bytes) and compute-\nbound codes (operations in ”ops.” column refer to ﬂops).\nimplemented by multiple tuning parameters (e. g., if op-\ntimizations are applied to multiple buﬀers or multiple di-\nmensions independently). Several benchmarks have been\nexecuted with a smaller tuning space on Radeon Vega56\nbecause the AMD ROCm driver has been crashing with\nsome tuning conﬁgurations (mainly using vectors of size\n16 and higher loop unrolling factors). Those benchmarks\nare Direct Coulomb Summation, GEMM, and N-Body.\nThe tuning spaces of benchmarks have been deﬁned\nduring their development. We have not performed any a\nposterior adjustment of the tuning spaces based on the ex-\nperimental evaluation (e. g., removing poorly-performing\nconﬁgurations).\nTherefore, we are able to evaluate the\ndiﬃculty of searching tuning space without bias caused by\nexperimental knowledge of well- or poor-performing con-\nﬁgurations.\n4.3. Eﬃciency of Benchmarks\nIf we want to study autotuning spaces (especially con-\ncerning how hard it is to search them), we should ﬁrst\nprove that those spaces allow us to generate a code with\nhigh performance. Here, we demonstrate that our bench-\nmarks either reach performance close enough to theoreti-\ncal boundaries of the hardware or at least outperform the\nbaseline8 implementation signiﬁcantly. We do not evaluate\n2D Convolution here: it does not perform at peak perfor-\nmance, but it reaches state-of-the-art performance [35], so\nit can be considered eﬃcient. We also exclude 3D Fourier\nReconstruction – it is a memory latency-bound code, mak-\ning theoretical performance boundaries diﬃcult to evalu-\nate. However, it has been shown that the autotuned imple-\nmentation of our gather-based 3D Fourier Reconstruction\nsigniﬁcantly outperforms state-of-the-art scatter-based ap-\nproach [43].\nWe deﬁne the eﬃciency of a benchmark as the relative\nperformance of the benchmark with respect to the relevant\nhardware performance boundaries (memory or arithmetic\nthroughput). More precisely, we use:\neﬃciency = 100 · max(\nMEMops\ntime\nMEMpeak ,\nALUops\ntime\nALUpeak )\n(2)\nwhere time is the runtime of computation, MEMpeak and\nALUpeak is peak memory and arithmetic throughput of\nthe hardware9. The MEMops and ALUops are the num-\nber of memory or arithmetic operations which are essential\nto solve the task. In other words, we count the number\nof operations required to solve the problem, not the num-\nber of operations required to execute the algorithm (such\nas array indexing, communication or computations dupli-\ncated among work-items). For example, BiCG benchmark\nis a memory-bound code, which essentially needs to read\nthe input matrix A once. Therefore, even if the unfused\nimplementation reads it twice, the number of operations is\ncomputed as the size of the input matrix in bytes divided\nby the runtime of the implementation. The formulas for\ncomputing ALUﬂops or MEMops of the benchmarks are\ngiven in Table 4.\nFor all benchmarks, we have measured the performance\nwith suﬃciently large data as to fully utilize the GPUs.\nFor Batched GEMM, small matrices of size 16 × 16 have\nbeen used.\nThe eﬃciency of tuned implementations is given in Ta-\nble 5. The performance of Hotspot benchmark is not close\nto the theoretical peak, so we measure the speedup over\nRodinia implementation.\nThe number of steps per ker-\nnel invocation is exposed to the user as a parameter in\n8the implementation we used as a basis for our benchmark, e. g.,\nthe Rodinia’s Hotspot\n9Only half the memory bandwidth has been considered for dual\nIntel Xeon E5-2650 because OpenCL provides no mechanism to op-\ntimize for NUMA in the dual-socket system (pinning memory buﬀers\nand work-groups to NUMA nodes is not possible). Therefore the full\nsystem bandwidth is not available.\n11\n\nBenchmark\n2080Ti\n1070\n750\nK20\nVega56\nE5-2650\n5110P\nBiCG\n88.3%\n84.7%\n81.7%\n50.4%\n75.6%\n46.0%\n6.45%\nCoulomb 3D\n91.8%\n91.4%\n84.3%\n43.2%\n65.3%\n74.2%\n22.2%\nGEMM\n79.8%\n80.6%\n91.1%\n51.3%\n96.3%\n37.5%\n19.7%\nGEMM batched\n86.8%\n81.4%\n90.0%\n49.6%\n86.0%\n27.7%\n20.9%\nTranspose\n87.1%\n80.2%\n86.3%\n64.2%\n86.1%\n62.5%\n10.0%\nN-body\n89.7%\n86.6%\n87.7%\n40.6%\n82.2%\n77.7%\n29.9%\nReduction\n68.7%\n87.5%\n89.4%\n64.1%\n71.6%\n33.9%\n10.1%\nHotspot\n1.35×\n1.94×\n2.06×\n1.4×\n2.88×\n1.2×\n12.8×\nTable 5: Performance of benchmarks autotuned for various hardware devices. The performance relative to the theoretical peak of devices\n(see Table 4) is shown for all benchmarks except for Hotspot, which is compared to the baseline Rodinia implementation.\nRodinia’s implementation of Hotspot. To have a fair com-\nparison, we have searched for the best-performing number\nof steps manually, testing the same values which have been\ntested by KTT in the autotuned version. As we can see,\nthe performance on GPUs is very good in general. We can\nreach a performance close to the theoretical peak (75%\nor more) in most cases for all architectures except Kepler\n(Tesla K20), which is less eﬃcient than other architectures\nin all benchmarks. The performance on dual-CPU (Xeon\nE5-2650) and MIC (Xeon Phi 5110P) is often far from the\ntheoretical peak. The development of OpenCL compiler\nseems to be not of high priority for CPU-based systems\n(for example Xeon Phi is not supported in Intel OpenCL\nfrom 2015), so this result is not surprising.\nNote that the performance of Coulomb 3D and N-body\nbenchmarks has been computed diﬀerently for GeForce\nGTX 2080Ti: the Touring architecture seems to perform\ntranscendental functions in parallel to FP32 instructions.\nTherefore, we have excluded the reciprocal square root\nfrom the computation of overall ﬂoating-point operations,\nusing formulas 5ak3 and 19n2 for Coulomb 3D and N-body,\nrespectively (see Table 4).\nOtherwise, the performance\nwould be overestimated.\n4.4. Performance Portability\nIn this section, we evaluate the performance portability\nof benchmarks without re-tuning them – i. e., how bench-\nmarks perform if they are executed on a diﬀerent device\nthan they are tuned for. This evaluation has been per-\nformed as follows. We have tuned all benchmarks for all\ndevices d. Then, for each benchmark tuned for device di,\nwe have measured its performance on devices dj, j ̸= i.\nThe performance is computed as a percentage of the max-\nimal reachable performance for the device: 100 perf (dj)\nperf (di).\nLet us compute the performance portability between\nGeForce GTX 750 and GeForce RTX 2080Ti as an exam-\nple. When BiCG benchmark is tuned for GeForce GTX\n750, it reaches 65 GB/s, when tuned for GeForce RTX\n2080Ti, it reaches 544 GB/s.\nWhen the code tuned for\nGeForce RTX 2080Ti is executed on GeForce GTX 750, it\nreaches 54.6 GB/s, so the performance portability is 84%.\nWhen the code tuned for GeForce GTX 750 is executed\non GeForce 2080Ti, it reaches performance 381 GB/s, so\nthe performance portability is 70%.\nDue to vast number of combinations, we compact the\nresults in Table 6 in the following way: we show the aver-\nage with standard deviation and worst-case performances\n(i) across GPU architectures, (ii) when GPU code is exe-\ncuted on CPU-derived architecture (CPU and MIC) and\n(iii) when CPU or MIC code is executed on GPU archi-\ntecture.\nThe table clearly demonstrates that the performance\nis not portable in general. Although the average perfor-\nmance portability is not bad among GPUs, the worst-cases\nare showing that for some benchmarks, there are combi-\nnations of GPUs with very bad performance portability,\nsuggesting the autotuning should be re-executed for diﬀer-\nent architectures. This is in line with related work, such\nas [26, 20, 43]. The performance portability is much worse\nin the case when the benchmarks are tuned for a CPU or\nMIC and executed on a GPU and vice versa. The poor\nportability between GPUs and CPU or MIC emphasizes\nthe important role of tuning – although our benchmarks\ncannot reach peak performance on CPU or MIC, their per-\nformance is much higher than in case when the GPU-tuned\ncode is simply executed on a CPU or MIC.\nThis experiment also reveals a serious limitation of\nfunctional portability with OpenCL. OpenCL guarantees\nthe functional portability of the code if it can be executed\non a device. When a kernel uses more hardware resources\n(e. g., number of registers per work-item) than is available\non the device, it cannot be executed. It seems that this is\nthe case of ﬁnely-tuned kernels, which often use as many\nresources as possible. When such kernels are executed on\na device with a lower amount of resources, they fail. As it\nis shown in Table 6, this can happen when a code tuned\nfor CPU, MIC, or GPU is executed on a diﬀerent GPU.\n5. Dynamic Autotuning\nIn this section, we experimentally evaluate dynamic\nautotuning for two applications: batched GEMM and 3D\nFourier reconstruction. Moreover, we analytically deter-\nmine the potential of dynamic autotuning for the rest of\nthe benchmarks.\n5.1. Methodology\nRecall that with dynamic autotuning, the tuning space\nis explored at runtime during application execution. There-\n12\n\nGPU→GPU\nGPU→CPU/MIC\nCPU/MIC→GPU\nBenchmark\navg±stdev\nworst\nfailed\navg±stdev\nworst\nfailed\navg±stdev\nworst\nfailed\nBiCG\n89.0%±12.3%\n57%\n1\n44.1%±17%\n28%\n0\n38.8%±29.5%\n11%\n0\nConvolution\n79.4%±14.9%\n55%\n3\n56.9%±18.5%\n33%\n0\n10.0%±3.6%\n6%\n1\nCoulomb 3D\n95.8%±6.5%\n67%\n0\n84.8%±2.7%\n81%\n0\n23.3%±16.9%\n3%\n2\nGEMM\n83.6%±16.4%\n31%\n0\n18.6%±18.5%\n1%\n0\n22.3%±6.6%\n13%\n2\nGEMM batched\n85.4%±17%\n37%\n0\n68.2%±13.2%\n39%\n0\n76.7%±22.2%\n46%\n1\nHotspot\n80.3%±17.5%\n46%\n3\n70.3%±15.6%\n44%\n0\n65.1%±8.9%\n59%\n6\nTranspose\n85.0%±21.9%\n8%\n3\n51.0%±27.1%\n11%\n0\n34.7%±14.7%\n14%\n0\nN-body\n78.8%±24.2%\n2%\n3\n45.9%±30.1%\n0%\n0\n25.7%±15.6%\n6%\n2\nReduction\n88.4%±24%\n12%\n3\n53.1%±17.4%\n26%\n0\n68.3%±23.8%\n37%\n1\nFourier\n74.5%±30%\n31%\n0\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nTable 6: Relative performance of benchmarks ported across GPU architectures, from CPU/MIC to GPU and from GPU to CPU/MIC, without\nre-tuning. Avg±stdev denotes the average and standard deviation of the relative performance, worst shows the worst-case performance, and\nfailed shows the number of cases when some conﬁguration cannot be executed on a device. 3D Fourier Reconstruction has been executed on\nsamples of 128 × 128 pixels on NVIDIA GPUs except for K20.\nfore, the implementation variants are compiled and bench-\nmarked during application run-time, resulting in four sources\nof overhead:\n• compilation of OpenCL or CUDA kernels (each ex-\nplored tuning conﬁguration needs to be compiled by\nthe JIT compiler);\n• execution of slower kernels (slower kernels prolong\ntuning time even in non-blocking autotuning when\ntheir results are used for computation);\n• enforced global synchronization between tuning runs\n(during autotuning, execution of the tuning manip-\nulators is not overlapped, see Section 3.3);\n• testing kernel output (this step is optional).\nThese overheads are relevant during the tuning phase only\n(i. e., when new conﬁgurations are searched).\nHowever,\nwhen a suﬃcient number of tuned kernel invocations is\nperformed after the tuning, the overhead becomes negli-\ngible. Here, we want to know how long the dynamically-\ntuned code has to run to amortize the tuning overhead\nunder a certain value. Or, alternatively, if dynamic auto-\ntuning reaches better performance than a code that has\nbeen oﬄine-tuned for a diﬀerent device or input data.\nNote that the overhead of the KTT API (mainly the\nexecution of manipulator in function runKernel) is negli-\ngible during the execution of the tuned code.\n5.2. Batched GEMM\nIn the previous section, we have introduced an auto-\ntuned kernel for batched multiplication of very small ma-\ntrices. This kernel is tuned for ﬁxed sizes of matrices, e. g.,\nwe have used matrices of size 16 × 16 for experiments in\nSection 4. However, the space of the possible matrix sizes\nis large. The GEMM kernel performs C = A · B, where\nA is an i × j matrix, B a k × i matrix, and C a k × j\nmatrix. Considering small matrices of sizes up to 32 in\neach dimension i, j, k, we get 32,768 combinations of the\nsizes. Consider an application or library, which does not\nknow the sizes of multiplied matrices before it is executed.\nIt would be impractical to oﬄine tune the application or\nlibrary for all possible sizes, so dynamic tuning, performed\nat runtime once the matrix size is ﬁxed, is of high practical\nvalue.\n5.2.1. Implementation\nWe have prepared an experiment, which simulates a\nreal application changing the matrix size from time to\ntime. Our testing application10 executed the tunable im-\nplementation of batched GEMM introduced in Section 4.2.5,\nbut it periodically changes the size of matrices and per-\nforms dynamic tuning.\nMore precisely, the application\ncomputes batched GEMM in a loop and randomly changes\nsizes i, j, k ∈[2, 32] every 30 seconds.\nThe application\ndoes not save the results of dynamic autotuning, so every\ntime the new sizes are used, the autotuning starts from\nscratch. The batch size has been selected so that matrices\noccupy approximately 900 MB of memory (so enough par-\nallelism is also exploited with very small matrices). When\nthe size of matrices is changed, the dynamic tuning using\nrandom search starts and is performed until (i) a conﬁg-\nuration reaching 75 % of the peak memory bandwidth is\nfound, or (ii) 20 conﬁgurations have been explored. The\nﬁrst rule allows the application to stop tuning when a con-\nﬁguration resulting in a suﬃcient performance is reached\n(we call such a conﬁguration as well-performing conﬁgu-\nration). The purpose of the second rule is to stop tuning\nwhen a conﬁguration performing close to the theoretical\npeak cannot be easily found (or does not exist at all). Af-\nter the tuning is stopped, the computation with the fastest\ntuning conﬁguration continues until the sizes of matrices\nare changed again. With an application changing matrices\nsizes less often, the tuning time could be prolonged. We\nhave measured the performance without tuning overhead\n(showing whether eﬃcient kernels can be found under lim-\nited tuning budget) and with tuning overhead (showing\n10https://github.com/Fillo7/KTT/blob/master/examples/\ngemm_batch/demo.cpp\n13\n\nthe real performance of the dynamically tunned applica-\ntion). The time required for initialization and copying of\nnewly created matrices was not benchmarked.\n5.2.2. Evaluation\nDevice\nMaximum\nRestricted\nIncl. overhead\nE5-2650\n24.5 GB/s\n88.6%\n82.9%\n5110P\n22.9 GB/s\n82.1%\n72.1%\nK20\n91.2 GB/s\n92.7%\n61.3%\nGTX 750\n63.0 GB/s\n91.4%\n87.8%\nGTX 1070\n205.7 GB/s\n97.2%\n94.3%\nVega 56\n308.5 GB/s\n86.2%\n74.4%\n2080Ti\n523.4 GB/s\n92.6%\n85.3%\nTable 7: Dynamically tuned Batched GEMM on diﬀerent computa-\ntional devices. The second column (Maximum) shows the average\nperformance of the fastest conﬁgurations (average for all tested ma-\ntrix sizes). The third column (Restricted) shows averaged relative\nperformance (relative to the maximum) of conﬁgurations reachable\nwith dynamic tuning under limited tuning budget (at most 20 con-\nﬁgurations explored). Finally, the fourth column (Incl. overhead)\nshows the averaged relative performance of dynamically tuned code,\nincluding tuning overhead.\nWe have run the experiments for 3,000 seconds (i. e.\n100 changes of matrix sizes) with all devices used in this\npaper. The matrix sizes have been selected randomly, but\nthe same sizes are used for all devices. We have also per-\nformed oﬄine tuning with exhaustive search for those sizes\nto obtain performance of the fastest conﬁguration at each\ndevice. The performance with and without tuning over-\nhead is computed as the relative performance of the fastest\nconﬁguration found by the oﬄine tuning. The results av-\neraged over all matrix sizes are shown in Table 7. The\ncode executing on dual Xeon E5-2650, Xeon Phi 5110P\nand Tesla K20, is compiled on Xeon E5-2650, which has\nquite poor single-core performance and therefore requires\na longer time for compilation. The prolonged compilation\ntime does not limit performance on CPU and MIC signiﬁ-\ncantly, because average kernels’ runtime is high and there-\nfore, the compilation does not induce signiﬁcant overhead.\nOn the other hand, the compilation overhead is quite no-\nticeable with Tesla K20. The batched GEMM kernel per-\nforms in general very well (i. e., it is close to the theoreti-\ncal peak) on GPUs except for Tesla K20. Its performance\nwith overhead is also quite close to the peak kernel per-\nformance (85% or better) in case of GeForce GTX 750,\nGeForce GTX 1070 and GeForce RTX 2080Ti, using Core\ni7-8700 for compilation. A bigger gap between the perfor-\nmance of the fastest kernels discovered during the tuning\nand performance with overhead can be seen with Radeon\nRTX Vega 56 (running in a system with Ryzen 7 1700).\nThe main reason is the higher number of poorly performing\nconﬁgurations and, therefore, more tuning steps required\nto ﬁnd a well-performing conﬁguration (i. e., the conﬁgura-\ntion within 75% of peak memory bandwidth, which leads\nto ﬁnalization of the tuning).\nFor better illustration of tuning performance, the ﬁrst\n300 s of the benchmark execution is shown for well-per-\n 0\n 50\n 100\n 150\n 200\n 250\n 0\n 50\n 100\n 150\n 200\n 250\n 300\nkernel perf.\nperf. with overhead\noffline tuned\nFigure 2:\nPerformance of dynamically tuned batched GEMM on\nGeForce GTX1070 + Core i7-8700. The sizes of matrices are changed\nevery 30 s. Performance of actually executed kernels is depicted as\ndots, whereas lines show performance including overhead. The max-\nimal performance reachable via oﬄine tuning with exhaustive search\nis shown as horizontal red lines.\n 0\n 20\n 40\n 60\n 80\n 100\n 120\n 140\n 0\n 50\n 100\n 150\n 200\n 250\n 300\nkernel perf.\nperf. with overhead\noffline tuned\nFigure 3:\nPerformance of dynamically tuned batched GEMM on\nTesla K20 + Xeon E5-2650. The sizes of matrices are changed every\n30 s. Performance of actually executed kernels is depicted as dots,\nwhereas lines show performance including overhead. The maximal\nperformance reachable via oﬄine tuning with exhaustive search is\nshown as horizontal red lines.\nforming GeForce GTX 1070 in Figure 2 and for Tesla K20,\nwhich suﬀers from tuning overhead, in Figure 3. Naturally,\nthe performance including tuning overhead drops, when a\nnew matrix size is used and increases in time as tuning\noverhead is amortized. It can be seen that GeForce GTX\n1070, coupled with the modern processor Core i7-8700, is\ncapable of amortizing tuning overhead in a very short time\n– after 30 seconds of execution, performance with tuning\noverhead is close to peak kernel performance. Even if mul-\ntiple conﬁgurations are searched before a well-performing\none is found (see performance between 270 and 300 sec-\nonds for GeForce GTX 1070 in Figure 2), the performance\nwith tuning overhead is close to the performance of the\nbest kernel found during the autotuning.\nOn the other\nhand, Tesla K20 cannot reach high performance for many\nmatrix sizes.\nThis also means that more conﬁgurations\n14\n\n2080Ti\n1070\n750\n680\n2080Ti\n100%\n99%\n31%\n49%\n1070\n99%\n100%\n31%\n50%\n750\n43%\n67%\n100%\n94%\n680\n60%\n72%\n71%\n100%\nTable 8: Performance portability of 3D Fourier reconstruction with\n128×128 samples. The rows represent GPUs used for oﬄine tuning;\nthe columns represent GPUs used for execution.\nThe percentage\nshows how performance diﬀers compared to the code using the best\ncombination of tuning parameters (for example, the code tuned for\nGeForce GTX 1070 and executed on GeForce GTX 750 runs at only\n31% of the speed of the code both tuned and executed on GeForce\nGTX 750).\n128x128\n91x91\n64x64\n50x50\n32x32\n128x128\n100%\n100%\n77%\n70%\n32%\n91x91\n100%\n100%\n76%\n68%\n33%\n64x64\n94%\n94%\n100%\n91%\n67%\n50x50\n79%\n78%\n98%\n100%\n86%\n32x32\n65%\n67%\n80%\n92%\n100%\nTable 9: Performance portability on GeForce GTX1070. The rows\nrepresent samples resolution used for oﬄine tuning, the columns rep-\nresent samples resolution used for execution. The percentage shows\nrelative performance compared to the code autotuned for the used\nresolution.\nare searched during the autotuning process. The system\nwith K20 uses an older Xeon E5-2650, which also pro-\nlongs kernel compilation time. Therefore, the overhead of\nthe tuning process is signiﬁcant and would need more ker-\nnel invocations to amortize.\nTesla K20 is also not able\nto reach high performance under limited tuning budget\n(see the diﬀerence between kernel performance and per-\nformance reachable by oﬄine tuning between 210 s and\n240 s, or between 270 s and 300 s for example).\n5.3. 3D Fourier reconstruction in Xmipp\nWe have introduced the CUDA-based GPU accelera-\ntion of the 3D Fourier reconstruction in [43], where KTT\nhas been integrated into the 3D Fourier reconstruction for\noﬄine tuning and the values of the tuning parameters for\nvarious hardware have been manually exported into the\nproduction code. The implementation requires autotun-\ning to maintain performance portability across GPUs (see\nTable 6). Because we were unable to install Xmipp on a\nsystem with Tesla K20, we have run the benchmark also\non GeForce GTX 680 to get more comprehensive results.\nDetailed performance portability across hardware de-\nvices is in Table 8. The size of the samples inserted into\na 3D domain inﬂuence the selection of optimal tuning pa-\nrameters. The tuning has to be repeated for samples of\ndiﬀerent sizes. Otherwise, suboptimal performance is ob-\ntained, as can be seen in Table 9.\n5.3.1. Implementation\nThe pseudocode of the reconstruction is shown in Al-\ngorithm 1. The output volumes G (Fourier transform of\nthe volume) and W (weights for the 3D voxels) are ini-\ntialized at the beginning of the computation. In the loop\nbody (lines 4-6), the samples are added into the 3D vol-\nume. More precisely, the samples are packed into buﬀers\nof a predeﬁned size, and their Fourier transform is com-\nputed on a CPU (line 4), copied into GPU memory (line\n5) and then tuned GPU kernel is executed to insert the\nsamples into volumes G, W (line 6). The whole algorithm\nis discussed in detail in [43].\nALGORITHM 1: 3D reconstruction\nInput: s\nOutput: G, W\n1 zero-initialize output volumes G, W in GPU memory;\n2 initialize buﬀer of image’s Fourier Transform sf in GPU\nmemory;\n3 foreach s ∈S do\n4\nsf ←FFT(s) on CPU;\n5\nupload sf to GPU;\n6\ninsert projecions of sf into G, W;\n7 end\n8 download G, W to CPU memory;\n9 apply weights W to G and perform inverse transform of G;\nWe use the non-blocking dynamic autotuning, which\nperforms both tuning and computation at the same time.\nTherefore, all input and output data have to be man-\naged by KTT during the whole program execution.\nIn\nthe loop in line 3, the data are prepared on CPU, then\na KTT method tuneKernelByStep, which launches one\nstep of dynamic tuning, is executed. The method selects a\nnew combination of the tuning parameters and executes a\ntuning manipulator. The tuning manipulator implements\nlines 5 and 6 of the algorithm. It ﬁrst copies buﬀer sf into\nGPU memory and then executes a kernel, which inserts\nsamples from sf into volumes G, W. The CPU code is mul-\ntithreaded, allowing to overlap computation of FFTs with\nkernel execution. The manipulator uses CUDA streams, so\nwhen tuning is ﬁnished (and therefore global parallelism is\nallowed, see Section 3.3) copying buﬀers may be executed\nin parallel with kernel execution and even multiple ker-\nnels may be executed in parallel (especially when process-\ning small samples). There is a tuning parameter changing\nwhether atomic writes to output volume in global memory\nare allowed (see Section 4.2.10). Depending on the tuning\nparameter value, the tuning manipulator method executes\nkernel iteratively for each projection (atomic writes are\ndisabled), or just once (processing all samples in buﬀer sf\nin one kernel call).\n5.3.2. Evaluation\nWe have designed an experiment demonstrating the us-\nability of dynamic autotuning with the 3D Fourier recon-\nstruction. We have used a real-world setup, performing\nreconstruction of the Brome Mosaic Virus [46] (EMPIAR\nentry 10010), processing 1,826,160 samples in resolution\n156 × 156. GPU kernels are processing 1500 samples at\n15\n\nbest runtime\ntuning 50\ntuning full\n2080Ti\n1m40s\n88% ± 3%\n54%\n1070\n5m49s\n96% ± 2%\n79%\n750\n16m59s\n92% ± 4%\n72%\n680\n15m12s\n94% ± 2%\n75%\nTable 10: The relative performance of dynamically-tuned 3D Fourier\nreconstruction. The best runtime is measured with ¯or¯aculum, i. e.,\nthe fastest kernel is selected immediately, and no tuning is performed.\nThe relative performance of tuning with searching 50 conﬁgurations\nand with searching the entire tuning space is measured as a per-\ncentage of the best runtime. Results for ”tuning 50” are shown as\nan average and standard deviation, whereas other results are shown\nas an average only (their performance is very stable across multiple\nexecutions).\nonce [43]; therefore, about 1280 kernels are executed to\nsolve the reconstruction (the actual number can be slightly\nhigher due to a small percentage of void samples). All ex-\nperiments with diﬀerent GPUs have been performed on a\ndesktop machine with Intel Core i7-8700.\nIn our experiment, the tuning is performed at the be-\nginning of the computation, when both used hardware and\nsample size are known. The performance of the dynami-\ncally tuned code is compared to the performance of code\nwith ¯or¯aculum (i. e., when the optimal tuning conﬁgura-\ntion obtained by the oﬄine tuning using exhaustive search\nis known at the beginning of the computation). We have\nmeasured dynamically tuned code in two settings. First,\nwe let KTT perform 50 search steps with random search\nand then continue with the fastest kernel explored. Sec-\nond, we perform the exhaustive search and continue with\nthe optimal conﬁguration. As the random search was used,\nthe experiment has been repeated 100 times. Results of\nthis experiment are shown in Table 10. As we can see, the\nperformance penalty of dynamic tuning is smaller than\nthe performance penalty we get for a code that was tuned\noﬄine for a diﬀerent hardware device (see Table 8) or dif-\nferent input size (see Table 9). The performance obatined\nwith dynamic tuning ranges between 88% and 96% of the\nperformance of code with ¯or¯aculum when 50 conﬁgura-\ntions are searched, whereas the code mistuned for diﬀerent\nGPU can perform at 31% of ¯or¯aculum in the worst case\n(see Table 8) and the code tuned for diﬀerent input size\ncan perform at 32% of ¯or¯aculum in the worst case (see\nTable 9).\nWe further analyze the overhead of dynamic autotun-\ning. Obviously, the more executions of the kernel (in our\ncase, the more samples used to reconstruct the 3D vol-\nume), the less overhead of dynamic autotuning. Therefore,\nfor more complicated reconstructions, the performance of\ndynamically tuned code is closer to the code using the\n¯or¯aculum, whereas trivial reconstruction may suﬀer from\ndynamic tuning overhead. Adding more work per kernel\n(in our case using larger samples) decreases relative over-\nhead of the compilation, but not the overhead caused by\nthe execution of slower kernels and synchronization.\nIn our experiment, the JIT compiler runs for 45.5 seconds\nwhen the full tuning space is searched. It introduces sig-\nniﬁcant overhead in the experiment with GeForce RTX\n2080Ti, as the GPU is very fast – the whole reconstruction\nwith ¯or¯aculum is ﬁnished in 1 minute 40 seconds. With all\nGPUs, some slowdown is caused by executing slow kernels.\nThe performance of average kernel is at 45% of the fastest\nkernel for RTX 2080Ti, 69% for GeForce GTX 1070, 46%\nfor GeForce GTX 750 and 52% for GeForce GTX 680. The\ngood average performance on GeForce GTX 1070 improves\nthe high relative speed of dynamic tuning with 50 explored\nconﬁgurations.\nWe have also measured the overhead of enforced global\nsynchronization. Recall that in such case, the tuning ma-\nnipulator copies input samples to the GPU and executes\nGPU kernels without the overlay with another manipula-\ntor instance.\nThe overhead is small for 3D Fourier re-\nconstruction: for kernels executed with enforced global\nsynchronization, it is 7% for GeForce RTX 2080Ti and\nGeForce GTX 1070, and 5% for GeForce GTX 750 and\nGeForce GTX 680. The global synchronization is enforced\nwhen kernels are tuned, but is not needed when tuning is\nﬁnished. Thus, in our setup, out of the total 1280 kernel\nexecutions, only those 50 launched by the tuning manipu-\nlator were slowed down.\nTo conclude, even if the reconstruction program runs\nin minutes only, dynamic tuning is able to reach better\nperformance than oﬄine tuning in the case oﬄine tuning\nwas performed for diﬀerent hardware, or diﬀerent input\nsize.\n5.4. Dynamic tuning of the benchmark set\nThe suitability for dynamic tuning for all benchmark\ncan be estimated analytically. We can compare the perfor-\nmance of the best kernel with the average performance of\nall kernels produced by the tuning space, which allows us\nto compute the overhead caused by executing slower ker-\nnels. We cannot evaluate the relative overhead of kernel\ncompilation, as it depends on application workload (large\nkernel input prolongs kernel runtime, whereas compila-\ntion time remains the same).\nWe also cannot consider\nthe overhead caused by the enforced global synchroniza-\ntion during tuning as it is highly application-dependent if\noverlapping of manipulators can be leveraged. The perfor-\nmance penalty of enforced synchronization, as well as ker-\nnels compilation, is similar for all tuning variants, whereas\nthe performance penalty of slow kernels can be much higher\n(some tuning variants can produce orders of magnitude\nslower kernels).\nTherefore, we consider the overhead of\nvery slow kernels as the most signiﬁcant one.\nIn the following we show how to estimate the number\nn of kernel invocations required in order to amortize the\ntuning overhead such that the performance of n kernel in-\nvocations including the dynamic tuning overhead is a cer-\ntain fraction of the performance we would have achieved\nby executing the application using a well-performing ker-\nnel n times. We deﬁne a well-performing conﬁguration as a\nconﬁguration producing a kernel with a performance with\n16\n\nwhich we are satisﬁed. Note that the well-performing con-\nﬁguration can be easily determined with some benchmarks\n(e. g., when deﬁned as a percentage of relevant hardware\ntheoretical peak), but it can be also virtually impossible\nto identify a well-performing conﬁguration until the whole\ntuning space is searched (e. g., when deﬁned as a conﬁgu-\nration reaching a certain fraction of the best conﬁguration\nperformance). In this section, we use a well-performing\nconﬁguration as a theoretical concept, which is used to\ndetermine the number of steps needed to amortize over-\nhead of dynamic tuning.\nLet the application with ¯or¯aculum be such an appli-\ncation where a well-performing conﬁguration is known at\nthe beginning of its execution (e. g., obtained by previ-\nously performed oﬄine tuning). Let the required perfor-\nmance of the dynamically tuned application relative to the\nperformance of the application with ¯or¯aculum be rp (so\nrp = 1.0 means that the dynamically tuned application\nruns at the same speed as the application that uses some\nwell-performing kernel found during oﬄine tuning). Let\nthe number of tuning steps be s, the average runtime of\nkernels within the tuning space be tavg and the runtime\nof the well-performing kernel be twell. Then, an average11\nvalue of rp is computed as:\nrp = s · tavg + (n −s) · twell\nn · twell\n(3)\nThe average number of kernel invocations n required\nto reach relative performance rp can be estimated as:\nn =\nrp · s · ( tavg\ntwell −1)\n1 −rp\n(4)\nFor example, if the average kernel has runtime tavg =\n10 ms, the well-performing kernel has runtime twell = 5 ms,\nwe perform s = 100 tuning steps and we want to reach rela-\ntive performance rp = 0.9 (i. e., dynamic autotuning reach\n90% of the performance of an application with ¯or¯aculum),\nwe need to perform 900 kernel invocations (including those\nused for tuning).\nThe real amortization of dynamic autotuning depends\non the number of tuning steps required to ﬁnd a well-\nperforming kernel. When random search is used, the num-\nber of required tuning steps can be computed as follows.\nLet r be the ratio of well-performing conﬁgurations in the\ntuning space and p be the required probability of ﬁnding\na well-performing conﬁguration.\nThe number of tuning\nsteps s, which leads to reaching the well-performing con-\nﬁguration with probability p, can be computed as\ns = log1−r(1 −p)\n(5)\nFor example, if the ratio of well-performing conﬁgura-\ntions is r = 0.01, then we need to explore 230 conﬁgu-\nrations in order to reach a well-performing conﬁguration\nwith probability p = 0.9.\n11Here, rp is computed for the average situation with s tuning steps\nand random search. Obviously, the tuning time may be diﬀerent from\ns · tavg in particular executions.\nUsing Equations 4 and 5, we can compute the number\nof kernel invocations needed to hide overhead caused by ex-\necuting slow kernels during dynamic tuning. We have set\nup the following experiment. We deﬁne a well-performing\nconﬁguration as a conﬁguration, which leads to at least\n95% of the best conﬁguration performance.\nUsing data\ngathered from the oﬄine tuning of our benchmark set with\nexhaustive search, we have computed the number of tun-\ning steps required to ﬁnd a well-performing conﬁguration\nwith probability 0.9 (using Equation 5). Then, we have\ncomputed the number of kernel executions required to de-\ncrease dynamic tuning overhead under 10% (i. e. to ob-\ntain at least 90% of the performance we would have with\n¯or¯aculum). The results are given in Table 11.\nTable 11 demonstrates that dynamic tuning is feasible\neven for short program executions (with thousands or tens\nof thousands of kernel calls) with multiple benchmarks,\nsuch as BiCG, Direct Coulomb Summation, Batched GEMM,\nHotspot, Transpose, N-body, Reduction and 3D Fourier\nReconstruction. Longer execution is needed for 2D Con-\nvolution and GEMM benchmarks. This test also shows\nsome interesting diﬀerences between the hardware devices\nused in the test. For example, autotuning of OpenCL code\nfor a CPU is similarly demanding as for GPUs with many\nbenchmarks, whereas it is much harder on the MIC (Xeon\nPhi) in multiple cases. There are also some benchmarks\nwhere diﬀerent hardware performs highly diﬀerently. For\nexample, with Batched GEMM on GeForce GTX 1070,\n304 conﬁgurations out of 424 are within 95% of the op-\ntimum and the average kernel performance is at 95% of\nthe optimum, so it is very easy to ﬁnd a well-performing\nkernel and to amortize tuning overhead. With GeForce\n750, only 40 conﬁgurations produce well-performing ker-\nnels, and the average kernel performance is within 62% of\nthe best one, so tuning is harder than for GeForce GTX\n1070. With Xeon E5-2650, only three tuning conﬁgura-\ntions result in a well-performing Batched GEMM kernel,\nand the average kernel performance is at 51% of the best\nkernel, so searching a well-performing kernel and amortiz-\ning the tuning overhead is signiﬁcantly harder on the CPU.\nInteresting diﬀerences between GPU and CPU/MIC can\nbe seen in 3D Coulomb Summation benchmark, where tun-\ning for GPUs is harder. Not only is the number of well-\nperforming kernels diﬀerent (e. g., 110 for Xeon E5-2650\nand 28 for GeForce GTX 1070), but a more signiﬁcant\ndiﬀerence is the average performance – it is much lower\nfor all GPUs (e. g., 5% of the best one on GeForce GTX\n1070 vs. 57% on Xeon E5-2650). The poor average speed\non GPU is caused by huge register spilling when high un-\nrolling of the inner loop is used.\nAlthough it would be\neasy to remove these slow-performing conﬁgurations from\nthe tuning space, we decided to keep the space as it was\ndesigned when the benchmark was developed instead of\nadding a posteriori information for tuning.\n17\n\nBenchmark\n2080Ti\n1070\n750\nK20\nVega56\nE5-2650\n5110P\nBiCG\n10,383\n9,425\n33.090\n43,552\n42,499\n32,338\n516,783\n2D Convolution\n265,297\n98,966\n197,550\n165,783\n99,087\n7,211\n3,435\nCoulomb 3D\n17,305\n16,346\n4,911\n5,289\n117*\n150\n631\nGEMM\n20,309\n151,564\n764,485\n205,122\n18,782*\n384,309\n3,106,384\nGEMM batched\n2\n2\n110\n214\n440\n2,341\n1,214\nHotspot\n4,314\n4,467\n3,309\n5,635\n1,489*\n3,926\n7,346\nTranspose\n9,398\n347\n2,998\n1,347\n140,177\n5,129\n60,688\nN-body\n7,539\n33,553\n2,531\n20,694\n554*\n2,472\n1,669,559\nReduction\n646\n78\n40\n218\n2,198\n1,650\n19,425\n3D Fourier\n2,239\n830\n3,123\nN/A\nN/A\nN/A\nN/A\nTable 11: The number of kernel invocations required to hide overhead of slow kernels execution. The goal is to ﬁnd a kernel within 95% of\nthe optimum with 90% probability and decrease tuning overhead under 10% of the runtime. Benchmarks on Radeon RX Vega56 marked with\n* are running with smaller tuning space due to ROCm instability.\n6. Conclusion and Future Work\nIn this paper, we have introduced the Kernel Tuning\nToolkit – an advanced autoning framework for OpenCL\nand CUDA applications. Using KTT allows expert pro-\ngrammers to conﬁgure applications for oﬄine tuning and\ndynamic tuning based on arbitrary user-deﬁned code op-\ntimization parameters. We have also developed a set of\nten benchmarks covering important HPC application ar-\neas and demonstrated that autotuning with KTT allows\nto produce highly eﬃcient implementations (often close to\nthe theoretical peak of the hardware) of these benchmarks\nfor diﬀerent hardware architectures including CPUs, Xeon\nPhi co-processors and GPUs. In our experimental eval-\nuations we also demonstrate that autotuning for diﬀerent\narchitectures is key for performance portability. Moreover,\nwe have shown that rationally-designed tuning spaces are\noften small enough to be searched during application run-\ntime, making dynamic tuning feasible for a subset of the\nconsidered benchmarks. We have demonstrated with two\ndiﬀerent applications that dynamic tuning outperforms of-\nﬂine tuned implementations quickly if some performance-\nrelevant aspects, such as a the size of data structures,\nchange. Moreover, we have shown that our framework can\nbe integrated into production software, supporting multi-\nthreading, overlapping execution of host and device code\nwith memory copies, and utilizing simultaneous kernel ex-\necution.\nIn future work, we would like to focus on the further\ndevelopment and integration of advanced search strategies.\nWe believe that it is possible to accelerate dynamic tuning\nby gathering properties of the tuning space from previous\ntuning runs, e. g., determine the relative impact of tuning\nparameters on performance by analyzing of proﬁling data.\nUsing more eﬃcient search methods would make dynamic\nautotuning feasible also for larger tuning spaces with a\nsmall number of well-performing conﬁgurations. Another\nline of research will focus on advanced dynamic strategies\nthat can detect when the performance of an application\ndegrades and that can then automatically trigger dynamic\nre-tuning of the code.\nAnother planned improvement targets the generation\nof tuning spaces. Currently, KTT ﬁrst generates the whole\ntuning space and then prunes it based on the constraints\ngiven. We plan to speed-up tuning space generation simi-\nlarly as it has been done in the ATF framework [36].\nFurthermore, we plan to investigate the possibilities of\nconnecting KTT with a compiler-based approach. By in-\ntroducing a DSL for optimizations, the programmer would\nneed to only implement advanced optimizations (such as\nchanging memory layout or the algorithm), whereas sim-\npler optimizations (such as vectorization or loop blocking)\nwould be generated automatically by the compiler.\nThe vast amount of autotuning results, especially when\ncoupled with proﬁling counters, can be used by the com-\nmunity to compare behavior and eﬃciency of diﬀerent HW\narchitectures, study eﬀects of diﬀerent code optimizations,\nor to develop new search strategies. Therefore, we plan to\nset up a public database containing tuning results with\nproﬁling counters and update this database with any new\nhardware or benchmark available.\nLast but not least, KTT has been designed to be in-\ndependent of the concrete API used for accelerated ker-\nnels (e. g., OpenCL or CUDA). It is, therefore, possible to\nadd broader support for APIs, for example, Vulcan sup-\nport would extend potential applications of KTT towards\ncomputer graphics. With non-blocking dynamic tuning,\nit would be possible to alter shaders at runtime without\nsigniﬁcant drop of frame rate.\nAcknowledgements\nThe work was supported from European Regional Develop-\nment Fund-Project ”CERIT Scientiﬁc Cloud” (No. CZ.02.1.01-\n/0.0/0.0/16 013/0001802). The project that gave rise to these\nresults received the support of a fellowship from la Caixa Foun-\ndation (ID 100010434). The fellowship code is LCF/BQ/DI18-\n/11660021. This project has received funding from the Euro-\npean Unions Horizon 2020 research and innovation programme\nunder the Marie Skodowska-Curie grant agreement No. 713673.\nThe Spanish Ministry of Economy and Competitiveness through\nGrants BIO2016-76400-R(AEI/FEDER, UE). Comunidad Aut-\nnoma de Madrid through Grant: S2017/BMD-3817.\n18\n\nReferences\n[1] Mart´ın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo,\nZhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jef-\nfrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow,\nAndrew Harp, Geoﬀrey Irving, Michael Isard, Yangqing Jia,\nRafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Lev-\nenberg, Dandelion Man´e, Rajat Monga, Sherry Moore, Derek\nMurray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit\nSteiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent\nVanhoucke, Vijay Vasudevan, Fernanda Vi´egas, Oriol Vinyals,\nPete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and\nXiaoqiang Zheng. TensorFlow: Large-scale machine learning on\nheterogeneous systems, 2015. Software available from tensor-\nﬂow.org.\n[2] V. Abrishami, J. R. Bilbao-Castro, J. Vargas, R. Marabini,\nJ. M. Carazo, and C. O. S. Sorzano.\nA fast iterative con-\nvolution weighting approach for gridding-based direct fourier\nthree-dimensional reconstruction with correction for the con-\ntrast transfer function. Ultramicroscopy, 157:79 – 87, 2015.\n[3] J. Ansel, S. Kamil, K. Veeramachaneni, J. Ragan-Kelley, J. Bos-\nboom, U.-M. O’Reilly, and S. Amarasinghe. OpenTuner: An\nextensible framework for program autotuning. In Proceedings\nof the 23rd International Conference on Parallel Architectures\nand Compilation, PACT ’14, pages 303–316, 2014.\n[4] E. Bajrovic and S. Benkner.\nAutomatic performance tuning\nof pipeline patterns for heterogeneous parallel architectures. In\n2014 International Conference on Parallel and Distributed Pro-\ncessing, Techniques and Applications, 2014.\n[5] E.\nBajrovic,\nMijakovic\nR.,\nJ.\nDokulil,\nS.\nBenkner,\nand\nM. Gerndt.\nTuning OpenCL applications with the periscope\ntuning framework. In 2016 49th Hawaii International Confer-\nence on System Sciences (HICSS), 2016.\n[6] P. Balaprakash, J. Dongarra, T. Gamblin, M. Hall, J. K.\nHollingsworth, B. Norris, and R. Vuduc. Autotuning in high-\nperformance computing applications. Proceedings of the IEEE,\n106(11):2068–2083, Nov 2018.\n[7] S. Che, M. Boyer, J. Meng, D. Tarjan, Tarjan, J. W. Sheaﬀer,\nS. Lee, and K. Skadron. Rodinia: A benchmark suite for het-\nerogeneous computing. In IEEE International Symposium on\nWorkload Characterization (IISWC), 2009.\n[8] C. Cummins, P. Petoumenos, Z. Wang, and H. Leather. End-\nto-end deep learning of optimization heuristics. In 2017 26th\nInternational Conference on Parallel Architectures and Com-\npilation Techniques (PACT), pages 219–232, 2017.\n[9] A. Danalis, G. Marin, C. McCurdy, J. S. Meredith, P. C. Roth,\nK. Spaﬀord, V. Tipparaju, and J. S. Vetter. The scalable hetero-\ngeneous computing (SHOC) benchmark suite. In Proceedings of\nthe 3rd Workshop on General-Purpose Computation on Graph-\nics Processing Units, GPGPU-3, pages 63–74. ACM, 2010.\n[10] J. J. Dongarra, J. Du Croz, S. Hammarling, and I. S. Duﬀ. A\nset of level 3 basic linear algebra subprograms. ACM Trans.\nMath. Softw., 16(1):1–17, 1990.\n[11] I. S. Duﬀand J. K. Reid. The multifrontal solution of indeﬁnite\nsparse symmetric linear. ACM Trans. Math. Softw., 9(3):302–\n325, 1983.\n[12] J. Enmyren, U. Dastgeer, and C. W. Kessler. Towards a tunable\nmulti-backend skeleton programming framework for multi-GPU\nsystems. In MCC-3: Swedish Woekshop on Multicore Comput-\ning, 2010.\n[13] T. L. Falch and A. C. Elster.\nMachine learning based auto-\ntuning for enhanced OpenCL performance portability. In Pro-\nceedings of the 2015 IEEE International Parallel and Dis-\ntributed Processing Symposium Workshop, 2015.\n[14] J. Filipoviˇc, M. Madzin, J. Fousek, and L. Matyska. Optimiz-\ning CUDA code by kernel fusion: application on BLAS. The\nJournal of Supercomputing, 2015.\n[15] J. Filipoviˇc, F. Petroviˇc, and S. Benkner.\nAutotuning of\nOpenCL kernels with global optimizations. In Proceedings of\nthe 1st Workshop on AutotuniNg and aDaptivity AppRoaches\nfor Energy Eﬃcient HPC Systems (ANDARE ’17), 2017.\n[16] M. Frigo and S. G. Johnson. The design and implementation of\nﬀtw3. Proceedings of the IEEE, 93(2):216–231, 2005.\n[17] D. Gadioli, R. Nobre, P. Pinto, E. Vitali, A. H. Ashouri,\nG. Palermo, J. Cardoso, and C. Silvano. SOCRATES a seam-\nless online compiler and system runtime autotuning framework\nfor energy-aware applications. In 2018 Design, Automation Test\nin Europe Conference Exhibition (DATE), 2018.\n[18] M. Gerndt, S. Benkner, E. C´esar, C. Navarrete, E. Bajrovic,\nJ. Dokulil, C. Guill´en, R. Mijakovic, and A. Sikora. A multi-\naspect online tuning framework for HPC applications. Software\nQuality Journal, 2017.\n[19] M. Gerndt, E. Cesar, and S. Benkner.\nAutomatic tuning of\nhpc applications - the periscope tuning framework (ptf).\nIn\nAutomatic Tuning of HPC Applications - The Periscope Tuning\nFramework (PTF). Shaker Verlag, 2015.\n[20] S. G. D. Gonzalo, S. D. Hammond, C. R. Trott, and W. M.\na. Hwu.\nRevisiting online autotuning for sparse-matrix vec-\ntor multiplication kernels on next-generation architectures. In\n2017 IEEE 19th International Conference on High Perfor-\nmance Computing and Communications; IEEE 15th Inter-\nnational Conference on Smart City; IEEE 3rd International\nConference on Data Science and Systems (HPCC/SmartCi-\nty/DSS), 2017.\n[21] D. S. Goodsell, G. M. Morris, and A. J. Olson.\nAutomated\ndocking of ﬂexible ligands: Applications of autodock. Journal\nof Molecular Recognition, 9(1):1–5, 1996.\n[22] S.\nGrauer-Gray\nand\nL.\nN.\nPouchet.\nPolybench/gpu\n1.0.\nhttp://web.cse.ohio-state.edu/~pouchet.2/software/\npolybench/GPU/index.html, 2012.\n[23] S. Grauer-Gray, L. Xu, R. Searles, S. Ayalasomayajula, and\nJ. Cavazos. Auto-tuning a high-level language targeted to gpu\ncodes. In 2012 Innovative Parallel Computing (InPar), 2012.\n[24] D. Grewe and A. Lokhmotov.\nAutomatically generating and\ntuning GPU code for sparse matrix-vector multiplication from\na high-level representation.\nIn Fourth Workshop on General\nPurpose Processing on Graphics Processing Units (GPGPU),\n2011.\n[25] T. Kisuki, P. M. W. Knijnenburg, and M. F. P. O’Boyle. Com-\nbined selection of tile sizes and unroll factors using iterative\ncompilation. In Proceedings 2000 International Conference on\nParallel Architectures and Compilation Techniques (PACT’00),\n2000.\n[26] J. Kurzak, S. Tomov, and J. Dongarra.\nAutotuning GEMM\nkernels for the Fermi GPU. IEEE Transactions on Parallel and\nDistributed Systems, 23(11):2045–2057, 2012.\n[27] Y. Li, J. Dongarra, and S. Tomov.\nA note on auto-tuning\nGEMM for GPUs. In Proceedings of the 9th International Con-\nference on Computational Science: Part I, 2009.\n[28] Y. Li, Y.-Q. Zhang, Y.-Q. Liu, G.-P. Long, and H.-P. Jia.\nMPFFT: An auto-tuning FFT library for OpenCL GPUs. Jour-\nnal of Computer Science and Technology, 28(1):90–105, 2013.\n[29] K. Ljungkvist. Matrix-free ﬁnite-element operator application\non graphics processing units. In Euro-Par 2014: Parallel Pro-\ncessing Workshops, pages 450–461. Springer International Pub-\nlishing, 2014.\n[30] I. Masliah, A. Abdelfattah, A. Haidar, S. Tomov, M. Baboulin,\nJ. Falcou, and J. Dongarra. High-performance matrix-matrix\nmultiplications of very small matrices. In Euro-Par 2016: Par-\nallel Processing, pages 659–671. Springer International Publish-\ning, 2016.\n[31] K. Matsumotoi, N. Nakasato, and S. G. Sedukhin. Performance\ntuning of matrix multiplication in OpenCL on diﬀerent GPUs\nand CPUs. In 2012 SC Companion: High Performance Com-\nputing, Networking Storage and Analysis, 2012.\n[32] R. Miceli, G. Civario, A. Sikora, E. C´esar, M. Gerndt, H. Haitof,\nC. Navarrete,\nS. Benkner,\nM. Sandrieser,\nL. Morin,\nand\nF. Bodin. AutoTune: A Plugin-Driven Approach to the Auto-\nmatic Tuning of Parallel Applications, pages 328–342. Springer,\n2013.\n[33] S. Muralidharan, A. Roy, M. Hall, M. Garland, and P. Rai.\nArchitecture-adaptive code variant tuning.\nSIGARCH Com-\n19\n\nputer Architecture News, 44(2):325–338, 2016.\n[34] S. Muralidharan, M. Shantharam, M. Hall, M. Garland, and\nB. Catanzaro. Nitro: A framework for adaptive code variant\ntuning.\nIn Proceedings of the 2014 IEEE 28th International\nParallel and Distributed Processing Symposium, IPDPS ’14,\npages 501–512. IEEE Computer Society, 2014.\n[35] C. Nugteren and V. Codreanu. CLTune: A generic auto-tuner\nfor OpenCL kernels. In Proceedings of the IEEE 9th Interna-\ntional Symposium on Embedded Multicore/Many-core Systems-\non-Chip (MCSoC), 2015.\n[36] A. Rasch and S. Gorlatch. ATF: A generic directive-based au-\ntotuning framework. Concurrency and Computation: Practice\nand Experience, 0(0):e4423, 2018.\n[37] A. Rasch, M. Haidl, and S. Gorlatch.\nATF: A generic auto-\ntuning framework. In 2017 IEEE 19th International Confer-\nence on High Performance Computing and Communications;\nIEEE 15th International Conference on Smart City; IEEE 3rd\nInternational Conference on Data Science and Systems (HPC-\nC/SmartCity/DSS), 2017.\n[38] G. Rudy, M. M. Khan, M. Hall, C. Chen, and J. Chame. A pro-\ngramming language interface to describe transformations and\ncode generation. In Languages and Compilers for Parallel Com-\nputing. Springer Berlin Heidelberg, 2011.\n[39] K. Seymour, H. You, and J. Dongarra. A comparison of search\nheuristics for empirical code optimization. In 2008 IEEE Inter-\nnational Conference on Cluster Computing, 2008.\n[40] M. Steuwer, T. Remmelg, and C. Dubach. LIFT: A functional\ndata-parallel ir for high-performance GPU code generation. In\n2017 IEEE/ACM International Symposium on Code Genera-\ntion and Optimization (CGO), pages 74–85, 2017.\n[41] J. E. Stone, J. C. Phillips, P. L. Freddolino, D. J. Hardy, L. G.\nTrabuco, and K. Schulten. Accelerating molecular modeling ap-\nplications with graphics processors. Journal of Computational\nChemistry, 28(16), 2007.\n[42] J. A. Stratton, C. Rodrigues, I. J. Sung, N. Obeid, L. W.\nChang, N. Anssari, G. D. Liu, and W. M. Hwu. Parboil: A re-\nvised benchmark suite for scientiﬁcand commercial throughput\ncomputing. Technical report, University of Illinois at Urbana-\nChampaign, 2012.\n[43] D. Stˇrel´ak, C. O. S. Sorzano, J. M. Carazo, and J. Filipoviˇc. A\nGPU acceleration of 3D Fourier reconstruction in Cryo-EM. The\nInternational Journal of High Performance Computing Appli-\ncations, 0, 2019.\n[44] A. Tiwari and J. K. Hollingsworth. Online adaptive code gen-\neration and tuning. In IEEE International Parallel Distributed\nProcessing Symposium (IPDPS), 2011.\n[45] V. Volkov and J. W. Demmel. Benchmarking GPUs to tune\ndense linear algebra. In ACM/IEEE conference on Supercom-\nputing (SC), 2008.\n[46] Z. Wang, C. F. Hryc, B. Bammes, P. V. Afonine, J. Jakana,\nD.-H. Chen, X. Liu, M. L. Baker, C. Kao, S. J. Ludtke, M. F.\nSchmid, P. D. Adams, and W. Chiu. An atomic model of brome\nmosaic virus using direct electron detection and real-space op-\ntimization. Nat Commun, 5:4808, 2014.\n[47] B. van Werkhoven. Kernel tuner: A search-optimizing gpu code\nauto-tuner.\nFuture Generation Computer Systems, 90:347 –\n358, 2019.\n[48] R. C. Whaley and J. J. Dongarra.\nAutomatically tuned lin-\near algebra software. In Proceedings of the 1998 ACM/IEEE\nConference on Supercomputing, 1998.\n20\n\n                </paper>\n                Optimize this CUDA kernel:\n                <kernel>\n                #include <cuda_runtime.h>\n\n// Naive matrix multiplication kernel\n__global__ void matmulKernel(float* A, float* B, float* C, int N) {\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < N && col < N) {\n        float sum = 0.0f;\n        for (int k = 0; k < N; k++) {\n            sum += A[row * N + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n\n// Host function to launch kernel\nvoid matmul(float* A, float* B, float* C, int N) {\n    // Define block and grid dimensions\n    dim3 blockDim(16, 16);\n    dim3 gridDim((N + blockDim.x - 1) / blockDim.x, \n                 (N + blockDim.y - 1) / blockDim.y);\n\n    // Launch kernel\n    matmulKernel<<<gridDim, blockDim>>>(A, B, C, N);\n}\n\n                </kernel>\n                Output the optimized CUDA kernel in <cuda></cuda> tags.\n                '}]}
2025-03-01 07:59:48 |     INFO | 
📝 Processing paper 2/2
2025-03-01 07:59:48 |     INFO | 📥 Downloading paper: 2501.09398.pdf
2025-03-01 07:59:50 |     INFO | 📄 Extracting text from PDF...
2025-03-01 07:59:50 |     INFO | ✓ Successfully processed paper (38484 chars extracted)
2025-03-01 07:59:50 |     INFO | 🤖 Implementing draft kernel optimization...
2025-03-01 07:59:50 |     INFO | Making request to DeepSeek API...
2025-03-01 08:00:51 |    ERROR | ❌ Network error during API request: HTTPSConnectionPool(host='api.deepseek.com', port=443): Read timed out.
2025-03-01 08:00:51 |    ERROR | Request payload: {'model': 'deepseek-reasoner', 'messages': [{'role': 'user', 'content': '\n                Using this paper as reference:\n                <paper>\n                Boosting Performance of Iterative Applications on\nGPUs: Kernel Batching with CUDA Graphs\nJonah Ekelund, Stefano Markidis, Ivy Peng\nKTH Royal Institute of Technology, Sweden\n{jonakek, markidis, bopeng}@kth.se\nAbstract—Graphics Processing Units (GPUs) have become\nthe standard in accelerating scientific applications on hetero-\ngeneous systems. However, as GPUs are getting faster, one\npotential performance bottleneck with GPU-accelerated appli-\ncations is the overhead from launching several fine-grained\nkernels. CUDA Graph addresses these performance challenges by\nenabling a graph-based execution model that captures operations\nas nodes and dependence as edges in a static graph. Thereby\nconsolidating several kernel launches into one graph launch.\nWe propose a performance optimization strategy for iteratively\nlaunched kernels. By grouping kernel launches into iteration\nbatches and then unrolling these batches into a CUDA Graph,\niterative applications can benefit from CUDA Graph for perfor-\nmance boosting. We analyze the performance gain and overhead\nfrom this approach by designing a skeleton application. The\nskeleton application also serves as a generalized example of\nconverting an iterative solver to CUDA Graph, and for deriving\na performance model. Using the skeleton application, we show\nthat when unrolling iteration batches for a given platform, there\nis an optimal size of the iteration batch, which is independent\nof workload, balancing the extra overhead from graph creation\nwith the performance gain of the graph execution. Depending\non workload, we show that the optimal iteration batch size\ngives more than 1.4× speed-up in the skeleton application.\nFurthermore, we show that similar speed-up can be gained in\nHotspot and Hotspot3D from the Rodinia benchmark suite and\na Finite-Difference Time-Domain (FDTD) Maxwell solver.\nIndex Terms—CUDA Graph, GPU Task Execution Model,\nGPU Performance Optimization, Hotspot3D, FDTD\nI. INTRODUCTION\nGraphics Processing Units (GPUs) have emerged as a major\nworkforce for a wide range of workloads, ranging from\ndeep learning [1] to scientific applications, such as molecular\ndynamics simulations [2], computational fluid dynamics [3]\nand quantum computer simulators [4]. However, as the GPU’s\ncomputational capabilities in terms of FLOPS/s increase, while\nthe execution time of the computational kernels decreases, the\ncost of kernel launching remains approximately constant. This\nis due to different factors not directly dependent on improved\nGPU micro-architectures. These factors include interaction\nwith the OS, driver and API constraints (which have not\nevolved as fast as the hardware), CPU-GPU communication\noverheads, and fixed latency in launch setup. To address this\nfundamental limitation due to the kernel launching cost, Nvidia\nintroduced CUDA Graph in CUDA 10, released in 2018.\nCUDA Graph consolidates multiple kernel launches into a\nThis work is supported by the EuroHPC Joint Undertaking project Plasma-\nPEPSC, project no. 101093261.\nsingle graph launch, reducing the overall launch overhead [5].\nThis graph-based approach helps minimize the repetitive setup\ncost by batching and organizing kernels. It reduces the de-\npendence on CPU-driven launches, fundamentally limiting the\napplication performance. Instead, it allows multiple kernels\nto be launched as a unified workload. In this work, we\ndevelop a performance model, port HPC applications to use\nCUDA Graph, and evaluate its performance benefits.\nA large number of scientific HPC applications are iterative\nin nature: they include a loop where the solver kernel is\noffloaded to the GPU at each iteration. Examples of such\napplications are all the applications based on time-stepping,\nsuch as electromagnetic solvers, particle pushers, Computa-\ntional Fluid Dynamics (CFD) solvers, to cite a few notable\nexamples. Iterative applications are ideal to use CUDA Graph\ncapabilities: the kernel launches at successive iterations can be\ngrouped in batches and use a single launch. In this work, we\ndevelop a performance model that shows that there should be\nan optimal size of the batches which balances performance\nimprovements with the added overhead of graph creation. We\nthen design and develop CUDA Graph ports of a skeleton\nversion of an iterative application, which then is used to\nvalidate a performance model, the HotSpot application, which\nmodels the thermal diffusion on a CPU, and an electromagnet-\nics application, based on the Finite Difference Time Domain\n(FDTD) numerical scheme. Using the skeleton application,\nwe perform a performance characterization of CUDA Graph\nperformance on Nvidia A100 GPU, varying the batch size, e.g.\nthe number of grouped kernels, the total number of iterations,\nand the total number of CUDA threads. The results are then\nvalidated on a Grace-Hopper system.\nThe goal of this work is to introduce a methodology to adopt\nCUDA Graph in iterative applications, with an associated per-\nformance model, and evaluate its performance improvement.\nOverall, we find that by applying this methodology, we can\nachieve performance improvements of up to 40%. The main\ncontributions of this work are the following:\n• The development of a methodology to use CUDA Graph\nin iterative applications, by grouping the kernel launches\ninto batches, and using CUDA Graph, see Fig. 1.\n• The design of a skeleton version of an iterative appli-\ncation using CUDA Graph and a performance model\nto understand the performance benefits of the proposed\napproach.\n• We provide a characterization study and a performance\narXiv:2501.09398v1  [cs.DC]  16 Jan 2025\n\nmodel of CUDA Graph in iterative applications, varying\nkey parameters like the batch size and number of itera-\ntions. The performance model and benchmarking can be\nused to estimate the parameters for the largest speed-up.\n• We detail all the steps to port an iterative application to\nuse CUDA Graph with code snippets and open-source\ncode.\nII. BACKGROUND\nToday, GPUs are a common component in both personal\ncomputers and supercomputers. Originally designed to process\ncomputer graphics, GPUs are specialized computing units built\nfor high data throughput by processing multiple data points\nwith the same instruction in parallel, i.e. Same Instruction\nMultiple Data (SIMD). This is useful in scientific applications\nwhere the same calculation is performed on multiple data\ninputs, such as simulations and neural networks.\nCUDA is a programming model and a platform for using\nNvidia GPUs as general computing devices in applications [6].\nIn CUDA, work for the GPU is defined as kernels launched to\nthe GPU. The kernel is executed in several threads specified at\nlaunch time, where each thread executes the same instruction\nbut can operate on different data. CUDA also has APIs for\nother operations, such as allocating memory in the GPU for\nuse in the application and copying data to and from the GPU.\nIn a task-graph execution model, operations are defined\nas nodes and dependencies between operations are explicitly\ncaptured as edges in a graph. CUDA Graph was introduced in\nCUDA 10 to enable such task graph execution model, where\nnodes represent operations including kernel launches, CPU\nfunction calls, memcpy or memset, allocation/de-allocation.\nWhen a sequence of kernels is captured in one static graph,\nthe CPU side only needs to launch the graph once, instead of\npaying the launch overhead for each kernel as in native CUDA\nprograms. Fig. 1 contrasts the execution timeline of an iterative\napplication executing with and without CUDA Graph. The\ntask graph execution model can reduce launch overhead and\nfree up CPU resources in applications, such as Deep Neural\nNetworks or simulations, where several short kernels are\nrelaunched many times [5]. However, when converting a native\nCUDA program into the task graph model, extra overhead\nis required for creating, initializing, and launching graphs.\nThus, this overhead requires quantitative consideration when\ncomposing the transformation strategy and the benefits of\nCUDA Graph in HPC application patterns.\nThere are two ways of constructing CUDA Graph. The\nfirst technique is called Stream Capture. It allows the capture\nof an existing CUDA stream implementation in a graph.\nThis is useful if the actual operations to be included in the\ngraph are in libraries, such as cuBLAS or cuSPARSE and not\ndirectly accessible to the programmer [7, 8]. Stream Capture\ncan be simple to implement, however, it requires that the\nstreams are synced by inserting CUDA events. A graph created\nwith the stream capture API can also be difficult to update\nonce it has been created [9–11]. The second approach is\nManual Graph Creation – the creation of the graph and adding\nof nodes to the graph is performed through manual CUDA\ncalls. This gives the developer precise control over the graph\ncreation and all dependencies. This requires the developer to\nknow all the configurations for the operations. It also takes\nmore effort to implement and can be harder to maintain [9].\nIn this work, we have chosen to use manual graph creation\ndue to the added control over the graph this enables.\nIII. METHODOLOGY\nIn this section, we first describe our iteration batch unrolling\nstrategy. We then describe the characterization and optimiza-\ntion tests performed on the skeleton application, followed by\nmeasurement and error calculations.\nA. Iteration Batch Unrolling into a Static Graph\nIn many scientific applications, the main computation phase\nconsists of many iterations of a small set of kernels, where a\ncommon implementation style is to call the CUDA kernel in\na loop for a specified number of iterations, see Listing 1.\nListing 1: Baseline implementation of an iterative application.\n1\nf o r\n( i n t\ni = 0;\ni < k e r n e l\ni t e r a t i o n s ; ++ i ) {\n2\nkernel<<<grid ,\nblock >>>(...) ;\n3 }\nConstructing a CUDA Graph with only one or two nodes is\nunlikely to yield any performance benefit when the kernels\nneed to be executed in sequence. On the other end, creating a\nCUDA Graph with a node for each call to launch the kernel\nwould result in an extensively large graph, with a large creation\noverhead. Therefore, to balance the added overhead of creating\nthe graph, we propose to first group a number of the kernel\nlaunches together into an iteration batch. Listing 2 shows how\nthis would look in the example in Listing 1. We then unroll\nthis iteration batch into a CUDA Graph which can be iterated\non. This iteration batch unrolling strategy is similar to loop\nunrolling in compiler optimizations.\nListing 2: Example of iteration batching in a traditional\nimplementaiton.\n1\nf o r\n( i n t\ni = 0;\ni < n r\ni t e r\nb a t c h e s ; ++ i ) {\n2\nf o r\n( i n t\nj = 0;\nj < i t e r a t i o n\nb a t c h\ns i z e ; ++ j ){\n3\nkernel<<<grid ,\nblock >>>(...) ;\n4\n}\n5 }\nBy unrolling the iteration batch into a CUDA Graph, we\ncan reduce the launch overhead by using fewer graph launches.\nFurthermore, presenting a larger workflow to the runtime can\nenable the runtime to perform more optimization on the execu-\ntion. Therefore, we should be able to amortize the cost of graph\ncreation and initialization and gain performance by reducing\nthe overall execution time. The challenge is determining a\nproper size for the iteration batch while the memory usage\nand performance are balanced. Based on our measurements,\nthe memory overhead of a CUDA Graph can be substantial\nif too many nodes are added.\nAn example of how the unrolling is implemented using the\nManual Graph Creation API to create the graph can be seen\n\nFig. 1: A schematic representation of the iteration batch unrolling strategy presented in this paper.\nin Listing 3. Note that this increases the Lines of Code (LoC)\nsignificantly compared to the implementation in Listing 1.\nListing 3: Graph implementation of an iterative application.\n1\ncudaGraphCreate ( graph , NULL) ;\n2\n3\nvoid * ka kernel [ ] =\n{ . . . } ;\n/ / Parameters\nf o r\nthe\nk e r n e l\n4\ncudaKernelNodeParams\nnp kernel = {0};\n5\nnp kernel . func = ( void\n*) k e r n e l ;\n6\nnp kernel . gridDim = g r i d ;\n7\nnp kernel . blockDim = block ;\n8\nnp kernel . kernelParams = ka kernel ;\n9\ncudaGraphNode t * l a s t\nn o d e = NULL;\n10\ni n t\nnum dependencies = 0;\n11\nf o r\n( i n t\ni = 0;\ni < i t e r a t i o n\nb a t c h\ns i z e ; ++ i ) {\n12\ncudaGraphAddKernelNode(&nodes [ i ] ,\ngraph ,\n13\nlast node ,\nnum dependencies ,\n14\n&np kernel ) ;\n15\nl a s t\nn o d e = &nodes [ i ] ;\n16\nnum dependencies = 1;\n17 }\n18\n19\nc u d a G r a p h I n s t a n t i a t e (&exec graph ,\ngraph ,\n20\ncudaGraphInstantiateFlagDeviceLaunch ) ;\n21\n22\ncudaStreamCreateWithFlags (& stream for graph ,\n23\ncudaStreamNonBlocking ) ;\n24\ncudaGraphUpload ( exec graph ,\nstream for graph ) ;\n25\n26\n/ /\nk e r n e l\ni t e r a t i o n s\nhave\nto\nbe\nd i v i s a b l e\n27\n/ /\nby\ni t e r a t i o n\nb a t c h\ns i z e\n28\ni n t\nn r\ni t e r\nb a t c h e s =\nk e r n e l\ni t e r a t i o n s /\ni t e r a t i o n\nb a t c h\ns i z e ;\n29\nf o r\n( i n t\ni = 0;\ni < n r\ni t e r\nb a t c h e s ; ++ i ) {\n30\ncudaGraphLaunch ( exec graph ,\nstream for graph ) ;\n31 }\nBuilding\nLaunching\nB. Performance Modeling\nAs shown in Fig. 1, harnessing CUDA Graph requires\na trade-off between the performance gain from the graph\nexecution and the increased overhead from graph creation.\nTherefore, when evaluating the execution performance of\nunrolling the iteration batch into a CUDA Graph we need\nto consider both the graph creation overhead TC and the\nexecution time TE\nT = TC + TE.\n(1)\nBased on how nodes are added in a linear sequence in\nListing 3, see the highlighted Building section, the graph\ncreation should be described by a linear equation\nTC = kcS + bc,\n(2)\nwhere S is the iteration batch size, kc and bc are the linear\ncoefficients. From Fig. 1 we can see that the execution TE,\ncan be described as the sum of the time to launch the first\ngraph Tl, the execution time of one graph Teg multiplied by\nthe number of iteration batches to launch I and the total time\nbetween the graph executions Ta(I −1):\nTE = Tl + (TkS + Ti(S −1))\n|\n{z\n}\nTeg\nI + Ta(I −1),\n(3)\nhere Teg is expanded as a sum between the execution time\nof all the kernels [TkS] and the time between kernel execu-\ntions in the graph [Ti(S −1)]. For a given number of total\nkernel executions (Ik) the relationship between the number of\niteration batches (I) and iteration batch size (S) is described\nby\nI = Ik\nS .\n(4)\nUsing this in Eq. 3 and rearranging we obtain\nTE = Ik(Ta −Ti)\nS\n+ Ik(Tk + Ti) −Ta + Tl.\n(5)\nEarly exploration indicated that the time between graphs is\nlonger than the time between kernels in the graph (Ta > Ti)\nand that Ta does not change with the graph size. Furthermore,\nas the graph is uploaded to the GPU before it is launched,\nline 24 in Listing 3, Tl should also be independent of the\ngraph size. According to to Eq. 5 we would therefore expect\nto obtain the best execution performance when the iteration\nbatch size is equal to the total number of kernel executions\n(S = Ik). If we then assume that switching from one workload\n(graph or kernel) launched to the GPU is approximately the\nsame (Ta ≈Tb in Fig. 1), executing the kernels as part of a\ngraph will give a speedup.\nHowever, this has to be balanced against the additional\ngraph creation overhead from Eq. 2. This suggests that there\nshould be an optimal size for the iteration batch independent\nof the kernel execution (Tk). If we define a = Ik(Ta −Ti)\nand b = Ik(Tk + Ti) −Ta + Tl, we can write Eq. 5 as\nTe = a\nS + b\n(6)\n\nwhere the coefficient a relates to the speed-up obtained by\nincreasing the number of nodes in the graph. For large Ik we\nhave that Ik(Tk + Ti) >> −Ta + Tl. Ta and Tl can thereby\nbe ignored and b will then represent a lower bound on the\npossible execution time where all kernels are part of one large\ngraph.\nOBSERVATION I: The iteration batch unrolling strategy\ncan achieve performance gains when the time to switch to\nthe next kernel inside the graph (Ti) is lower than the time\nto switch from one launched kernel to the next launched\nkernel (Ta).\nWe identify the value of the iteration batch S, such that\nEq. 1 is minimized, to optimize the performance. However,\nas the total number of kernel executions (Ik), the number of\niteration batches (I) and the iteration batch size (S) all are\nlimited integer numbers, Eq. 4 also limits the possible iteration\nbatch sizes for a given total number of kernel executions.\nC. Experimental Setup\nTo analyze the graph creation overhead TC and graph\nexecution TE, we designed a skeleton version of an iterative\napplication1, providing a generalized example of how an\niterative application can be converted to use CUDA Graph.\nThe computational workload in the skeleton application is a\nsimple vector multiplication kernel, multiplying a constant\nvalue to a vector. Through a command line interface, the\nexecution of the application can be changed to run either\nwith or without CUDA Graph. This enables a comparison\nbetween the baseline GPU implementation and the graph\nimplementation. Furthermore, the size of the iteration batch,\nthe number of iteration batches, and the number of threads per\nkernel can be changed through the command line interface.\nWhen running the baseline GPU implementation, the graph\nlaunch is replaced with a for-loop launching all the kernels in\nan iteration batch, similar to what is shown in Listing. 2. Using\nthe skeleton application, we test the overhead and performance\nof a CUDA Graph when increasing the size of the iteration\nbatch (for the graph version, this is the same as the number of\nnodes in the graph) while keeping the total number of kernel\nexecutions constant. By measuring the graph creation time and\nthe execution time, we evaluate Eqs. 2 and 6.\nTo assess how the memory footprint of the created graph\nscales with the iteration batch size, we measure the memory\nusage of the skeleton application for the increasing size of the\ngraph. This is done by calling CUDA Systems management\nInterface (nvidia-smi) from within the application after the\nexecution of the graph, but before the graph is destroyed.\nFinally, to find the optimal graph size we use the skeleton\napplication and measure the graph creation and total execution\ntime while increasing iteration batch size and keeping the\nnumber of required kernel executions (Ik) constant. Using the\noptimal graph size, we can then evaluate the speed-up of the\nskeleton application, Hotspot and an FDTD Maxwell solver.\n1Skeleton application: https://github.com/Jonah-E/IterApp\nThe Hotspot and Hotspot3D initial implementations used\nin this work are from the Rodinia benchmark suite [12].\nHotspot is a modeling tool for thermal analysis of Very Large-\nScale Integration (VLSI) systems, enabling early analysis of\nthermal properties [13].\nThe FDTD Maxwell solver is developed as part of this\nwork2. It solves the 3D Maxwell equations for a specified num-\nber of time steps for a 3D electromagnetics cavity problem.\nThis application is slightly different compared to the skeleton\napplication in that it launches two kernels, one for the H-Field\ncalculations and one for the E-Field.\nAll time and memory measurements in the skeleton appli-\ncation were performed ten times for each configuration and\nthe mean and standard deviation are shown in the figures. For\nthe speed-up test, the time is measured for the software parts\nthat differ between the graph and the baseline versions, e.g.,\nthe graph creation and graph/kernel launch and execution. The\nspeed-up is then calculated as the ratio between the mean value\nfor the baseline version and the graph version. The error for\nthe speed-up for an iteration batch size of S is calculated as\nthe square root of the summed squares of standard deviations\ndivided by the corresponding mean value.\nOur test bed system consists of an Nvidia A100 GPU with\n40 GB global memory with an AMD EPIC 7302P as the host.\nIt runs CentOS Linux 8 (Kernel: 4.18.0) with CUDA 12.3 with\nGPU Driver version 545.23.08, Nsight Systems 2023.3.3.42,\nnvidia-smi 545.23.08, nvcc v12.3.52, and gcc v8.5.0. We use\nNVIDIA Nsight Systems and System Management Interface\n(nvidia-smi) for collecting performance and related metrics for\nthe NVIDIA system.\nWhile most of the experiments are carried out on an Nvidia\nA100, we also test the iterative applications on a Grace-Hopper\nsystem consisting of a Grace CPU (72-core ARM Neoverse\nV2 CPU), and an Nvidia H100 GPU with 96 GB HBM3\nmemory. This system runs RHEL Linux 9.3 (Kernel: 5.14.0)\nwith CUDA 12.2.\nIV. RESULTS\nIn this section, we evaluate the overhead and execution of\nunrolling iteratively launched kernels into CUDA graphs using\nthe skeleton application. We then find the optimal size for the\niteration batch and evaluate the speed-up using the skeleton\napplication. Finally, using the results from these evaluations,\nwe will show the speed-up for Hotspot and Hotspot3D from\nthe Rodinia benchmark suite and an FDTD application.\nA. Skeleton Application: Overhead and Execution\nBy increasing the iteration batch size, we can measure the\ngraph creation overhead as a function of the number of graph\nnodes in the graph. This also includes uploading the graph to\nthe GPU. In Fig. 2 we can see that the graph creation time\nfollows close to a linear increase with respect to the size of\nthe iteration batch up to a size of 2,500. The coefficients kc\nand bc from Eq. 2 are listed in Table I, together with the\n2FDTD Maxwell solver: https://github.com/Jonah-E/FDTD\n\nFig. 2: Graph creation overhead and memory usage in the\nskeleton application as the size of the iteration batch increases.\nThe creation overhead is the line plot with the y-axis on the\nleft side and the memory usage is the bar plot with the y-axis\non the right.\nFig. 3: Launch and execution time for the graph version of\nthe skeleton application as the size of the iteration batch\nincreases. Note the logarithmic scale of the x-axis, and that\neach workload has a separate y-axis.\nMean Absolute Error (MEA). The slope for the creation time\nis similar between all three workloads, which is in line with\nour expectations. Above 2,500, the graph creation time does\nnot follow the same linear increase; however, the limitation\nin possible iteration batch sizes from Eq. 4 prevents the\nevaluation of alternative correlations. Interestingly, the creation\ntime for the largest workload, (Threads = 1e+07) has a slightly\nhigher base time than the other two, also seen in the bc value\nin Table I. We observe a similar offset in the GPU memory\nusage for the different workloads in Fig. 2, indicating that this\nincrease in creation time is associated with the graph upload\nto the GPU.\nTABLE I: Fit parameters and MAE\nGraph Creation, Eq 2\nGraph Execution, Eq. 6\nThreads\nkc\nbc\nMAE\na\nb\nMAE\n1e3\n4.18e-6\n1.59e-4\n9.41e-5\n1.77e-2\n4.56e-2\n1.3e-3\n1e6\n4.23e-6\n1.72e-4\n9.72e-5\n1.07e-2\n2.02e-1\n1.5e-3\n1e7\n4.27e-6\n4.22e-4\n7.28e-5\n5.62e-3\n1.40e+0\n2.1e-3\nFig. 4: Skeleton application performance for different iteration\nbatch sizes and 10,000 kernel iterations. The ratio between the\ngraph creation time plus the total execution time for each graph\nsize and the graph size with the lowest mean execution time.\nNote the logarithmic scale of the x-axis.\nUsing Nsight Systems, we can collect an execution trace\nof the graph execution in the skeleton application, Fig. 5.\nHere, we can see that the assumption for Eq. 4, that the time\nbetween graph executions is longer than the time between\nkernel executions inside the graph (Ti < Ta), holds true.\nOBSERVATION II: The execution trace from Nsight Sys-\ntems aligns with the expectation that Ti < Ta, presented in\nSection III-B.\nFig. 3 shows how the execution time Te changes as the size\nof the iteration batch increases, including lines fitted to Eq. 6,\nwith coefficient values in Table I. Each of the workloads has its\ny-axis with different offsets but the same scale. As expected,\nthe b values are different for the different workloads. Unex-\npectedly, a has different values for the different workloads.\nThis suggests that the difference between the time between\ngraphs and the time between kernel executions within graphs\nis smaller for larger workloads.\nSimilar to the graph creation time in Fig. 2, the execution\ntime in Fig. 3 does not follow the model for iteration batch\nsizes larger than 2,500 kernels. Instead of converging towards\na constant value, the execution time increases as the size of\nthe iteration batch is 5,000 or 10,000, that is containing either\n50% or 100% of the total kernel executions Ik.\nOBSERVATION III: Larger workloads have lower poten-\ntial performance gain from CUDA Graph.\nTo evaluate the optimal graph size and minimize Eqs. 1,\nwe can measure both the graph creation and execution time\ntogether. In Fig. 4, we can see this together with Eq. 1 using\nthe fitted parameters for Eq. 2 and 6. The data and fitted\nlines have been divided with the lowest mean time for each\nworkload. From this, it can be seen that the optimal size for the\niteration batch is around 50-100 kernels. However, the optimal\ngraph size is less important when the workload is larger. The\n\nFig. 5: Execution trace from Nsight Systems. The first two graphs executed on the GPU are enclosed in boxes.\nFig. 6: Skeleton application speedup of the graph version\ncompared to the baseline version without graph.\nreason for this is shown in Table I. First, the b coefficient is\nlarger for the larger workloads, meaning that the speed-up for\nincreasing the iteration batch size will be lower in proportion\nto the total execution time. Second, the a coefficient is smaller,\nindicating lower possible speed-ups for the larger workloads.\nThe memory usage should also be considered when select-\ning the optimal iteration batch size. Inspecting Fig. 2, the\nmemory usage increases linearly with the number of nodes in\nthe graph and agrees with results from previous works [10, 14].\nThis is the overall memory usage of the GPU, we therefore\ncannot evaluate the absolute values but only analyze the trend\nfor increasing graph size. Based on this, we can conclude that\nthe optimal iteration batch size is approximately 100 nodes.\nUsing a larger graph size will mean less optimal performance\nand higher memory usage.\nB. Performance\nSkeleton Application. With the optimal iteration batch size\nfrom Fig. 4 and using Eq. 4 we can calculate the number of\niteration batches to execute for the required kernel executions\nas I =\nIk\n100. Using this, we examine the performance gain for\ndifferent numbers of required kernel launches. Here we mea-\nsure the graph creation time and the total execution time of all\nthe launched graphs/kernels, we exclude common setup steps,\nsuch as allocating memory in the GPU, from the measurement.\nFig. 6 shows the speed-up of the graph version of the skeleton\napplication relative to the baseline version (without CUDA\nFig. 7: Speed-up for Hotspot and Hotspot3D when using\nCUDA Graph on the A100 System. The first number for all\nthe legend entries is the grid size, for the calculation grid. The\nsame size is used for both the number of columns and rows\nin the grid. Hotspot3D also has multiple layers, the number\nof layers are written after the x in the legend.\ngraph). We can see that for a low number of executed iteration\nbatches, there is no speed-up. However, there is a crossover\npoint at two to three iteration batches where the graph version\nbecomes faster compared to the baseline version. As expected\nfrom the a coefficient in Table I, the smallest workloads have\nthe highest possible performance gain with speed-up over 1.5\ntimes, while the higher the workload the lower the speed-up.\nHotspot and Hotspot3D. Using our iteration batch unrolling\nstrategy on the Hotspot and Hotspot3D benchmarks from\nRodinia, the speedup, shown in Fig. 7, has the same profile\nas the skeleton application. The larger the problem size is, the\nsmaller the speedup is compared to the total execution time.\nHowever, after ∼3 iteration batches all workloads are above\none, indicating that there at least is not a performance penalty.\nFDTD Maxwell Solver. Similar to both the skeleton and\nHotspot applications, a speed-up in the FDTD Maxwell solver,\nFig. 8, is visible. Exhibiting a speed-up of up to 1.2× for\nthe smaller workloads tested and no slowdown for larger\nworkloads.\nExecution on an Nvidia Grace-Hopper System. Executing\nthe skeleton application on the NVIDIA Grace Hopper system,\nFig. 9, we observe the same general trend for the different\nworkloads as on the A100 system, Fig. 6. However, the\n\nFig. 8: Speed-up for the FDTD Maxwell solver on the A100\nsystem.\nFig. 9: Skeleton application speedup of the graph version\ncompared to the baseline version without graph on an Nvidia\nGrace-Hopper system.\ncrossover point has moved forward, requiring a larger number\nof iteration batches for the CUDA Graph version to be faster.\nOBSERVATION IV: The tests using the skeleton applica-\ntion, Hotspot, Hotspot3D and the FDTD Maxwell solver\nshow that the iteration batch unrolling strategy can bring\nmore than 1.4× speed up for small workloads and have no\nperformance penalty for larger workloads.\nV. RELATED WORK\nHeterogeneous\nProgramming:\nPrior\nworks\nexplored\nCUDA Graph\nin\nheterogeneous\nprogramming\nthrough\nOpenMP\nand\nOpenACC.\nYu\net\nal.\n[9]\nproposed\nan\napproach to transform OpenMP task dependency graphs\ninto\nCUDA Graph,\nenabling\nthe\nuser\nto\nobtain\nthe\nperformance benefits from CUDA Graph while keeping\nthe programmability of OpenMP. Toledo et al. [15] proposes\nan\nintegration\nof\nCUDA Graph\ninto\nOpenACC,\nthey\npresent the benefits of this by using the Particle Swarm\nOptimization and integrating OpenACC-generated kernels\ninto a CUDA Graph. Our work focuses on one code pattern\nwhere one kernel is launched multiple times; the kernel\nunrolling into CUDA Graph presented in this work could be\ndone through directives such as those proposed by Yu et al.\n[9] and Toledo et al. [15].\nPerformance Improvement: Other works improve the per-\nformance of a given task graph. Qiao et al. [16] introduced\nCUDA Graph to an image processing DSL and proposed a\nkernel pipeline approach to increase parallelism to optimize a\ngraph based on resource usage. CudaFlow [10] is an abstrac-\ntion layer to simplify the usage of CUDA Graph; with the\nstream capture abstraction, they also introduce a scheduling\nalgorithm to improve the performance. Grape [14] is a graph\ncompiler integrated into PyTorch to improve the efficiency of\ngraph-based execution for dynamic DNNs on GPU. Grape also\nintroduces control flow logic for the graphs in the framework\nbut makes no use of conditional nodes. AutoGraph [17],\nis a framework for optimizing the computation graph of\nDeep Neural Networks (DNN) and utilizes CUDA Graph to\nreduce the overall launch cost. Opara [18] is a framework\ndesigned to increase the parallel execution of operators in\nDNN workloads. It creates a CUDA Graph based on profiling\ninformation of resource usage. Regarding using CUDA Graph\nin applications, Tang et al. [19] utilize CUDA Graph for\nspeech recognition running in a commercial setting. In contrast\nto these works focusing on the optimization of the computation\nby increasing the parallelism in the execution, our work\nfocuses on optimization without any parallelism where the\nfull workflow is too large to efficiently express as a graph.\nLin et al. [10] did show that the execution of a linear chain\nof nodes can be optimized for some performance gain over\nthe manually defined CUDA Graph for graphs with over 104\nnodes. However, they did not compare this to the performance\nwithout CUDA Graph.\nVI. DISCUSSION AND CONCLUSION\nIn this work, we presented a strategy to increase the per-\nformance of iteratively launched CUDA kernels by grouping\nthe kernel launches into iteration batches and then unrolling\nthe iteration batch into a CUDA Graph. The basis of this\nstrategy is the expectation that the CUDA runtime will be\nfaster at switching to the next node inside a graph than\nswitching independently launched workloads. We developed\na skeleton application and used it to quantify the overhead\nand possible performance gain with the strategy. Using the\nsame skeleton application, we showed that using an optimal\nsize for the iteration batch, over 1.4× speed-ups is achieved.\nFurthermore, we showed similar speedups in other applications\nand on different NVIDIA GPU architectures.\nWe showed that the graph creation time increases according\nto Eq. 2 up to an iteration batch size of 2,500. This means that\nfor the tested Ik, 25% of the number of nodes is contained\nin the graph. According to the model, Eq. 5, increasing the\niteration batch further would only constitute a minor decrease\nin execution time. The measurements of the actual execution\ntime show that it is even worse, as the execution time increases\ninstead of converges, as the iteration batch size increases\nover 2,500. However, we also see in Fig. 3 that this is less\n\npronounced for the smallest workload, indicating that this can\nbe connected to the resource usage of the graph.\nWith the skeleton application, we can use the same-sized\ngraph to increase the performance for all the workloads, even\nthe large problem sizes where the individual kernel execution\nis longer. This is primarily due to the graph creation time is\nonly marginally affected by the problem size, while there is\nonly a small difference in the performance profile between\nthe different workloads. Increasing the workload therefore\nonly marginally affects the absolute performance gain from\nusing CUDA Graph. However, as we have seen, the larger\nworkloads also mean that the possible performance gain from\nusing CUDA Graph is a smaller part of the total execution.\nConverting the Hotspot and Hotspot3D applications from\nRodinia as well as the FDTD Maxwell solver shows that this\nstrategy of unrolling iteration batches into CUDA Graph can\nalso be used on real applications. Where a speed-up of over\n1.4 is achieved for the smaller workloads. The performance\nin these applications further highlights the effect the workload\nsize has on the performance. If an application often executes\nan iterative loop with large workloads, it will not provide\na large performance benefit to convert the application to\nuse CUDA Graph. However, if the application has small\nworkloads, kernels with short execution times and small thread\ncounts, it can provide a significant benefit to convert the\nexecution to CUDA Graph using our strategy to unroll the\niteration batch. Even if the application occasionally executes\na large workload, the strategy will not incur any penalty.\nAs shown in this work, CUDA Graph can be used to\nincrease the performance of iterative applications. Three weak-\nnesses of the methodology presented are the total iterations of\nthe kernels are limited to be a multiple of the iteration batch\nsize, the use of manual graph creation which can be challeng-\ning to implement and maintain and it is not portable to systems\nwithout CUDA. The first weakness can be addressed using\nloop peeling. To address the second and third weaknesses,\nfuture work will include the iteration batch unrolling strategy\ninto a DSL or heterogeneous programming framework, where\nthe graph creation can be handled automatically. This could\nalso include automatic characterization and selecting the ap-\npropriate iteration batch size for optimal performance.\nREFERENCES\n[1]\nY. Bengio et al., Deep learning. MIT press Cambridge,\nMA, USA, 2017, vol. 1.\n[2]\nM. I. Andersson et al., “Breaking down the parallel\nperformance of gromacs, a high-performance molecular\ndynamics software,” in PPAM, Springer, 2022.\n[3]\nM. Karp et al., “Large-scale direct numerical simula-\ntions of turbulence using gpus and modern fortran,” The\nInternational Journal of High Performance Computing\nApplications, vol. 37, no. 5, 2023.\n[4]\nJ. Faj et al., “Quantum computer simulations at warp\nspeed: Assessing the impact of gpu acceleration: A case\nstudy with ibm qiskit aer, nvidia thrust & cuquantum,”\nin 2023 e-Science, IEEE, 2023.\n[5]\nP. Ramarao, CUDA 10 Features Revealed: Turing,\nCUDA Graphs, and More, Sep. 2018. [Online]. Avail-\nable: https : / / developer . nvidia . com / blog / cuda - 10 -\nfeatures-revealed/ (visited on 10/31/2024).\n[6]\nCUDA Zone - Library of Resources, Accessed: 2024-\n04-05. [Online]. Available: https : / / developer . nvidia .\ncom/cuda-zone.\n[7]\nNVIDIA, CUDA Graphs. [Online]. Available: https://\ndocs.nvidia.com/cuda/archive/12.3.0/cuda-c-programm\ning-guide/index.html (visited on 02/20/2024).\n[8]\nNVIDIA, Graph Management. [Online]. Available: htt\nps://docs.nvidia.com/cuda/cuda-runtime-api/group\nC\nUDART\nGRAPH.html#group\nCUDART\nGRAPH\n(visited on 02/20/2024).\n[9]\nC. Yu et al., “OpenMP to CUDA graphs: A compiler-\nbased transformation to enhance the programmability\nof NVIDIA devices,” in Proceedings of the 23th In-\nternational Workshop on Software and Compilers for\nEmbedded Systems, ACM, May 2020.\n[10]\nD.-L. Lin et al., “Efficient GPU Computation Using\nTask Graph Parallelism,” in Euro-Par 2021: Parallel\nProcessing, L. Sousa et al., Eds., vol. 12820, Springer\nInternational Publishing, 2021.\n[11]\nJ. Says, Constructing CUDA Graphs with Dynamic\nParameters, Aug. 2022. [Online]. Available: https : / /\ndeveloper.nvidia.com/blog/constructing-cuda-graphs-\nwith-dynamic-parameters/ (visited on 02/21/2024).\n[12]\nS. Che et al., “Rodinia: A benchmark suite for hetero-\ngeneous computing,” in 2009 IEEE International Sym-\nposium on Workload Characterization (IISWC), 2009.\n[13]\nW. Huang et al., “Hotspot: A compact thermal modeling\nmethodology for early-stage vlsi design,” IEEE Trans-\nactions on Very Large Scale Integration (VLSI) Systems,\nvol. 14, no. 5, 2006.\n[14]\nB. Zheng et al., “Grape: Practical and Efficient Graphed\nExecution for Dynamic Deep Neural Networks on\nGPUs,” in 56th Annual IEEE/ACM International Sym-\nposium on Microarchitecture, ACM, Oct. 2023.\n[15]\nL. Toledo et al., “Towards Enhancing Coding Pro-\nductivity for GPU Programming Using Static Graphs,”\nElectronics, vol. 11, no. 9, Jan. 2022.\n[16]\nB. Qiao et al., “The Best of Both Worlds: Combining\nCUDA Graph with an Image Processing DSL,” in 2020\n57th ACM/IEEE Design Automation Conference (DAC),\nIEEE, Jul. 2020.\n[17]\nY. Zhao et al., “AutoGraph: Optimizing DNN Com-\nputation Graph for Parallel GPU Kernel Execution,”\nProceedings of the AAAI Conference on Artificial In-\ntelligence, vol. 37, no. 9, Jun. 2023.\n[18]\nA. Chen et al., “Opara: Exploiting Operator Parallelism\nfor Expediting DNN Inference on GPUs,” 2023.\n[19]\nR. Tang et al., “SpeechNet: Weakly Supervised, End-\nto-End Speech Recognition at Industrial Scale,” 2022.\n\n                </paper>\n                Optimize this CUDA kernel:\n                <kernel>\n                #include <cuda_runtime.h>\n\n// Naive matrix multiplication kernel\n__global__ void matmulKernel(float* A, float* B, float* C, int N) {\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < N && col < N) {\n        float sum = 0.0f;\n        for (int k = 0; k < N; k++) {\n            sum += A[row * N + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n\n// Host function to launch kernel\nvoid matmul(float* A, float* B, float* C, int N) {\n    // Define block and grid dimensions\n    dim3 blockDim(16, 16);\n    dim3 gridDim((N + blockDim.x - 1) / blockDim.x, \n                 (N + blockDim.y - 1) / blockDim.y);\n\n    // Launch kernel\n    matmulKernel<<<gridDim, blockDim>>>(A, B, C, N);\n}\n\n                </kernel>\n                Output the optimized CUDA kernel in <cuda></cuda> tags.\n                '}]}
2025-03-01 08:00:51 |    ERROR | ❌ Error in optimization process: Failed to generate any optimized kernels
